{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.sparse import csc_matrix, csr_matrix\n",
    "import pdb\n",
    "import datetime\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from cifar_classifier import MaskedCifar\n",
    "from classifier import Classifier\n",
    "from mnist_classifier import MaskedMNist\n",
    "from pruning.methods import weight_prune, prune_rate, get_all_weights\n",
    "from pruning.utils import to_var\n",
    "from resnet import MaskedResNet18, MaskedResNet34, MaskedResNet50, MaskedResNet101, MaskedResNet152\n",
    "from classifier_utils import setup_default_args\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from configurations import configurations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weights(model):\n",
    "    weights = []\n",
    "\n",
    "    if len(list(model.children())) != 0:\n",
    "        for l in model.children():\n",
    "            weights += get_all_weights(l)\n",
    "    else:\n",
    "        for p in model.parameters():\n",
    "            if len(p.data.size()) != 1: # Avoid bias parameters\n",
    "                weights += list(p.cpu().data.abs().numpy().flatten())\n",
    "\n",
    "    return weights\n",
    "\n",
    "def gen_masks_for_layer(model, threshold):\n",
    "    # generate mask\n",
    "    for p in model.parameters():\n",
    "        if len(p.data.size()) != 1:\n",
    "            pruned_inds = p.data.abs() > threshold\n",
    "            return pruned_inds.float()\n",
    "    \n",
    "def gen_masks_recursive(model, threshold):\n",
    "    masks = []\n",
    "    \n",
    "    for module in model.children():\n",
    "        if 'Masked' not in str(type(module)):\n",
    "            print(\"Skipping masking of layer: \", module)\n",
    "            continue\n",
    "        if len(list(module.children())) != 0:\n",
    "            masks.append(gen_masks_recursive(module, threshold))\n",
    "        else:\n",
    "            masks.append(gen_masks_for_layer(module, threshold))\n",
    "    \n",
    "    return masks\n",
    "\n",
    "def weight_prune(model, pruning_perc):\n",
    "    '''\n",
    "    Prune pruning_perc% weights globally (not layer-wise)\n",
    "    arXiv: 1606.09274\n",
    "    '''    \n",
    "    all_weights = get_all_weights(model)\n",
    "    threshold = np.percentile(np.array(all_weights), pruning_perc)\n",
    "    return gen_masks_recursive(model, threshold)\n",
    "\n",
    "def prune_rate(model, verbose=True):\n",
    "    \"\"\"\n",
    "    Print out prune rate for each layer and the whole network\n",
    "    \"\"\"\n",
    "    total_nb_param = 0\n",
    "    nb_zero_param = 0\n",
    "\n",
    "    layer_id = 0\n",
    "\n",
    "    for parameter in model.parameters():\n",
    "\n",
    "        param_this_layer = 1\n",
    "        for dim in parameter.data.size():\n",
    "            param_this_layer *= dim\n",
    "        total_nb_param += param_this_layer\n",
    "\n",
    "        # only pruning linear and conv layers\n",
    "        if len(parameter.data.size()) != 1:\n",
    "            layer_id += 1\n",
    "            zero_param_this_layer = \\\n",
    "                np.count_nonzero(parameter.cpu().data.numpy()==0)\n",
    "            nb_zero_param += zero_param_this_layer\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Layer {} | {} layer | {:.2f}% parameters pruned\" \\\n",
    "                    .format(\n",
    "                        layer_id,\n",
    "                        'Conv' if len(parameter.data.size()) == 4 \\\n",
    "                            else 'Linear',\n",
    "                        100.*zero_param_this_layer/param_this_layer,\n",
    "                        ))\n",
    "    pruning_perc = 100.*nb_zero_param/total_nb_param\n",
    "    if verbose:\n",
    "        print(\"Final pruning rate: {:.2f}%\".format(pruning_perc))\n",
    "    return pruning_perc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "    config = [x for x in configurations if x['name'] == 'FCCifar10Classifier'][0]\n",
    "\n",
    "    model = config['model']()\n",
    "\n",
    "    device = 'cuda:2'\n",
    "\n",
    "    train_data = test_data = config['dataset'](\n",
    "        './data', train=True, download=True, transform=transforms.Compose(config['transforms'])\n",
    "    )\n",
    "\n",
    "    test_data = config['dataset'](\n",
    "        './data', train=False, download=True, transform=transforms.Compose(config['transforms'])\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True, num_workers=1, pin_memory=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=8, shuffle=True, num_workers=1, pin_memory=True)\n",
    "    optimizer = config['optimizer'](model.parameters(), lr=0.01, momentum=0.5)\n",
    "    \n",
    "    wrapper = Classifier(model, device, train_loader, test_loader)\n",
    "\n",
    "    model.load_state_dict(torch.load('./models/cifar_classifier.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_k_means(model, bits=5):\n",
    "    for module in model.children():\n",
    "        if 'weight' not in dir(module):\n",
    "            continue\n",
    "\n",
    "        dev = module.weight.device\n",
    "        weight = module.weight.data.cpu().numpy()\n",
    "        original_shape = weight.shape\n",
    "        weight = np.reshape(weight, (2, -1))\n",
    "        mat = csr_matrix(weight)\n",
    "        min_ = min(mat.data)\n",
    "        max_ = max(mat.data)\n",
    "        space = np.linspace(min_, max_, num=2**bits)\n",
    "        kmeans = KMeans(n_clusters=len(space), init=space.reshape(-1,1), n_init=1, precompute_distances=True, algorithm=\"full\")\n",
    "        kmeans.fit(mat.reshape(-1,1))\n",
    "\n",
    "        weight = kmeans.cluster_centers_[kmeans.labels_].reshape(original_shape)\n",
    "        weight_tensor = torch.tensor(weight, requires_grad=True).to(dev)\n",
    "\n",
    "        # Use register_hooks to recalculate the gradients\n",
    "        module.weight.data = weight_tensor\n",
    "        module.weight.register_hook(gen_param_hook(torch.from_numpy(kmeans.labels_), dev))\n",
    "\n",
    "def get_where_val_is_zero(array):\n",
    "    for i, val in enumerate(array):\n",
    "        if val == 0:\n",
    "            return i\n",
    "    \n",
    "    return None\n",
    "\n",
    "        \n",
    "def gen_param_hook(c_labels, dev):\n",
    "    \n",
    "    def hook(grad):\n",
    "        # print(f\"Retraining start time {datetime.datetime.now()}\")\n",
    "        grad_original_shape = grad.shape\n",
    "        reshape_start_time = datetime.datetime.now()\n",
    "        grads = grad.reshape(-1, 1)\n",
    "        reshape_end_time = datetime.datetime.now()\n",
    "\n",
    "#         print(f\"Reshape took: {reshape_end_time - reshape_start_time}\")\n",
    "\n",
    "        update_values = torch.tensor(np.zeros(shape=(len(c_labels), 1)), dtype=torch.float).to(dev)\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        enumartion_start_time = datetime.datetime.now()\n",
    "\n",
    "        for i, g in enumerate(grads):\n",
    "            cluster_id = c_labels[i].item()\n",
    "            update_values[cluster_id] += g\n",
    "        \n",
    "        \n",
    "        enumeration_end_time = datetime.datetime.now()\n",
    "\n",
    "        print(f\"Enumeration time took: {enumeration_end_time - enumartion_start_time}\")\n",
    "\n",
    "        updated_grads = torch.tensor(np.zeros(len(grads)), dtype=torch.float).to(dev)\n",
    "        \n",
    "        # For each grad\n",
    "        # Find the c_label\n",
    "        # Find all the grads with the same c_\n",
    "        \n",
    "        for i in range(len(grads)):\n",
    "            cluster_id = c_labels[i].item()\n",
    "            grad = update_values[c_labels[i].item()]\n",
    "            updated_grads[i] = grad\n",
    "            \n",
    "        updated_grads = updated_grads.reshape(grad_original_shape)\n",
    "\n",
    "        # print(f\"Retrain end time {datetime.datetime.now()}\")\n",
    "        end_time = datetime.datetime.now()\n",
    "        print(f\"Weight vector with {i} gradients took {end_time - start_time} to cluster gradient updates.\")\n",
    "\n",
    "        return updated_grads\n",
    "    \n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_k_means(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 0.697216\n",
      "Train Epoch: 0 [80/50000 (0%)]\tLoss: 1.843118\n",
      "Train Epoch: 0 [160/50000 (0%)]\tLoss: 1.488126\n",
      "Train Epoch: 0 [240/50000 (0%)]\tLoss: 1.329273\n",
      "Train Epoch: 0 [320/50000 (1%)]\tLoss: 0.517650\n",
      "Train Epoch: 0 [400/50000 (1%)]\tLoss: 1.389887\n",
      "Train Epoch: 0 [480/50000 (1%)]\tLoss: 0.940510\n",
      "Train Epoch: 0 [560/50000 (1%)]\tLoss: 1.525506\n",
      "Train Epoch: 0 [640/50000 (1%)]\tLoss: 0.746754\n",
      "Train Epoch: 0 [720/50000 (1%)]\tLoss: 3.038354\n",
      "Train Epoch: 0 [800/50000 (2%)]\tLoss: 1.187228\n",
      "Train Epoch: 0 [880/50000 (2%)]\tLoss: 1.709471\n",
      "Train Epoch: 0 [960/50000 (2%)]\tLoss: 1.020142\n",
      "Train Epoch: 0 [1040/50000 (2%)]\tLoss: 1.008796\n",
      "Train Epoch: 0 [1120/50000 (2%)]\tLoss: 1.484752\n",
      "Train Epoch: 0 [1200/50000 (2%)]\tLoss: 1.550498\n",
      "Train Epoch: 0 [1280/50000 (3%)]\tLoss: 1.325531\n",
      "Train Epoch: 0 [1360/50000 (3%)]\tLoss: 1.245444\n",
      "Train Epoch: 0 [1440/50000 (3%)]\tLoss: 2.028663\n",
      "Train Epoch: 0 [1520/50000 (3%)]\tLoss: 1.279292\n",
      "Train Epoch: 0 [1600/50000 (3%)]\tLoss: 1.753613\n",
      "Train Epoch: 0 [1680/50000 (3%)]\tLoss: 1.756375\n",
      "Train Epoch: 0 [1760/50000 (4%)]\tLoss: 1.155363\n",
      "Train Epoch: 0 [1840/50000 (4%)]\tLoss: 1.347932\n",
      "Train Epoch: 0 [1920/50000 (4%)]\tLoss: 0.921406\n",
      "Train Epoch: 0 [2000/50000 (4%)]\tLoss: 0.496249\n",
      "Train Epoch: 0 [2080/50000 (4%)]\tLoss: 1.319824\n",
      "Train Epoch: 0 [2160/50000 (4%)]\tLoss: 1.452877\n",
      "Train Epoch: 0 [2240/50000 (4%)]\tLoss: 1.053314\n",
      "Train Epoch: 0 [2320/50000 (5%)]\tLoss: 2.139122\n",
      "Train Epoch: 0 [2400/50000 (5%)]\tLoss: 1.231887\n",
      "Train Epoch: 0 [2480/50000 (5%)]\tLoss: 1.020964\n",
      "Train Epoch: 0 [2560/50000 (5%)]\tLoss: 1.522190\n",
      "Train Epoch: 0 [2640/50000 (5%)]\tLoss: 1.805382\n",
      "Train Epoch: 0 [2720/50000 (5%)]\tLoss: 1.616804\n",
      "Train Epoch: 0 [2800/50000 (6%)]\tLoss: 0.953822\n",
      "Train Epoch: 0 [2880/50000 (6%)]\tLoss: 1.382375\n",
      "Train Epoch: 0 [2960/50000 (6%)]\tLoss: 1.710235\n",
      "Train Epoch: 0 [3040/50000 (6%)]\tLoss: 1.747176\n",
      "Train Epoch: 0 [3120/50000 (6%)]\tLoss: 1.425568\n",
      "Train Epoch: 0 [3200/50000 (6%)]\tLoss: 1.214924\n",
      "Train Epoch: 0 [3280/50000 (7%)]\tLoss: 1.104007\n",
      "Train Epoch: 0 [3360/50000 (7%)]\tLoss: 0.848564\n",
      "Train Epoch: 0 [3440/50000 (7%)]\tLoss: 1.866063\n",
      "Train Epoch: 0 [3520/50000 (7%)]\tLoss: 2.068287\n",
      "Train Epoch: 0 [3600/50000 (7%)]\tLoss: 0.924349\n",
      "Train Epoch: 0 [3680/50000 (7%)]\tLoss: 2.142034\n",
      "Train Epoch: 0 [3760/50000 (8%)]\tLoss: 1.214151\n",
      "Train Epoch: 0 [3840/50000 (8%)]\tLoss: 1.166586\n",
      "Train Epoch: 0 [3920/50000 (8%)]\tLoss: 1.787668\n",
      "Train Epoch: 0 [4000/50000 (8%)]\tLoss: 1.257394\n",
      "Train Epoch: 0 [4080/50000 (8%)]\tLoss: 1.306109\n",
      "Train Epoch: 0 [4160/50000 (8%)]\tLoss: 2.150106\n",
      "Train Epoch: 0 [4240/50000 (8%)]\tLoss: 1.803754\n",
      "Train Epoch: 0 [4320/50000 (9%)]\tLoss: 0.734916\n",
      "Train Epoch: 0 [4400/50000 (9%)]\tLoss: 2.531575\n",
      "Train Epoch: 0 [4480/50000 (9%)]\tLoss: 1.346594\n",
      "Train Epoch: 0 [4560/50000 (9%)]\tLoss: 0.812185\n",
      "Train Epoch: 0 [4640/50000 (9%)]\tLoss: 0.984756\n",
      "Train Epoch: 0 [4720/50000 (9%)]\tLoss: 1.237093\n",
      "Train Epoch: 0 [4800/50000 (10%)]\tLoss: 0.890097\n",
      "Train Epoch: 0 [4880/50000 (10%)]\tLoss: 1.652806\n",
      "Train Epoch: 0 [4960/50000 (10%)]\tLoss: 1.999342\n",
      "Train Epoch: 0 [5040/50000 (10%)]\tLoss: 1.278402\n",
      "Train Epoch: 0 [5120/50000 (10%)]\tLoss: 1.644654\n",
      "Train Epoch: 0 [5200/50000 (10%)]\tLoss: 0.923592\n",
      "Train Epoch: 0 [5280/50000 (11%)]\tLoss: 0.688471\n",
      "Train Epoch: 0 [5360/50000 (11%)]\tLoss: 0.565351\n",
      "Train Epoch: 0 [5440/50000 (11%)]\tLoss: 1.105422\n",
      "Train Epoch: 0 [5520/50000 (11%)]\tLoss: 1.164964\n",
      "Train Epoch: 0 [5600/50000 (11%)]\tLoss: 1.486139\n",
      "Train Epoch: 0 [5680/50000 (11%)]\tLoss: 1.240433\n",
      "Train Epoch: 0 [5760/50000 (12%)]\tLoss: 0.415882\n",
      "Train Epoch: 0 [5840/50000 (12%)]\tLoss: 1.654503\n",
      "Train Epoch: 0 [5920/50000 (12%)]\tLoss: 1.491912\n",
      "Train Epoch: 0 [6000/50000 (12%)]\tLoss: 1.001453\n",
      "Train Epoch: 0 [6080/50000 (12%)]\tLoss: 1.658068\n",
      "Train Epoch: 0 [6160/50000 (12%)]\tLoss: 1.953210\n",
      "Train Epoch: 0 [6240/50000 (12%)]\tLoss: 1.005903\n",
      "Train Epoch: 0 [6320/50000 (13%)]\tLoss: 1.523069\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 1.365192\n",
      "Train Epoch: 0 [6480/50000 (13%)]\tLoss: 1.397027\n",
      "Train Epoch: 0 [6560/50000 (13%)]\tLoss: 1.246590\n",
      "Train Epoch: 0 [6640/50000 (13%)]\tLoss: 0.999301\n",
      "Train Epoch: 0 [6720/50000 (13%)]\tLoss: 1.911073\n",
      "Train Epoch: 0 [6800/50000 (14%)]\tLoss: 0.608862\n",
      "Train Epoch: 0 [6880/50000 (14%)]\tLoss: 1.338270\n",
      "Train Epoch: 0 [6960/50000 (14%)]\tLoss: 1.342141\n",
      "Train Epoch: 0 [7040/50000 (14%)]\tLoss: 0.940176\n",
      "Train Epoch: 0 [7120/50000 (14%)]\tLoss: 1.037347\n",
      "Train Epoch: 0 [7200/50000 (14%)]\tLoss: 1.375693\n",
      "Train Epoch: 0 [7280/50000 (15%)]\tLoss: 1.828660\n",
      "Train Epoch: 0 [7360/50000 (15%)]\tLoss: 2.153975\n",
      "Train Epoch: 0 [7440/50000 (15%)]\tLoss: 1.401204\n",
      "Train Epoch: 0 [7520/50000 (15%)]\tLoss: 0.966119\n",
      "Train Epoch: 0 [7600/50000 (15%)]\tLoss: 1.082168\n",
      "Train Epoch: 0 [7680/50000 (15%)]\tLoss: 1.061956\n",
      "Train Epoch: 0 [7760/50000 (16%)]\tLoss: 2.071868\n",
      "Train Epoch: 0 [7840/50000 (16%)]\tLoss: 1.322634\n",
      "Train Epoch: 0 [7920/50000 (16%)]\tLoss: 1.747143\n",
      "Train Epoch: 0 [8000/50000 (16%)]\tLoss: 1.752233\n",
      "Train Epoch: 0 [8080/50000 (16%)]\tLoss: 0.976588\n",
      "Train Epoch: 0 [8160/50000 (16%)]\tLoss: 1.296157\n",
      "Train Epoch: 0 [8240/50000 (16%)]\tLoss: 1.158196\n",
      "Train Epoch: 0 [8320/50000 (17%)]\tLoss: 1.040822\n",
      "Train Epoch: 0 [8400/50000 (17%)]\tLoss: 1.807773\n",
      "Train Epoch: 0 [8480/50000 (17%)]\tLoss: 0.663001\n",
      "Train Epoch: 0 [8560/50000 (17%)]\tLoss: 1.576596\n",
      "Train Epoch: 0 [8640/50000 (17%)]\tLoss: 1.453745\n",
      "Train Epoch: 0 [8720/50000 (17%)]\tLoss: 0.946706\n",
      "Train Epoch: 0 [8800/50000 (18%)]\tLoss: 1.375841\n",
      "Train Epoch: 0 [8880/50000 (18%)]\tLoss: 1.491437\n",
      "Train Epoch: 0 [8960/50000 (18%)]\tLoss: 1.379112\n",
      "Train Epoch: 0 [9040/50000 (18%)]\tLoss: 1.198194\n",
      "Train Epoch: 0 [9120/50000 (18%)]\tLoss: 0.891904\n",
      "Train Epoch: 0 [9200/50000 (18%)]\tLoss: 1.484831\n",
      "Train Epoch: 0 [9280/50000 (19%)]\tLoss: 1.296907\n",
      "Train Epoch: 0 [9360/50000 (19%)]\tLoss: 1.880163\n",
      "Train Epoch: 0 [9440/50000 (19%)]\tLoss: 0.630531\n",
      "Train Epoch: 0 [9520/50000 (19%)]\tLoss: 1.255850\n",
      "Train Epoch: 0 [9600/50000 (19%)]\tLoss: 1.194269\n",
      "Train Epoch: 0 [9680/50000 (19%)]\tLoss: 1.490311\n",
      "Train Epoch: 0 [9760/50000 (20%)]\tLoss: 1.290124\n",
      "Train Epoch: 0 [9840/50000 (20%)]\tLoss: 0.826438\n",
      "Train Epoch: 0 [9920/50000 (20%)]\tLoss: 0.695843\n",
      "Train Epoch: 0 [10000/50000 (20%)]\tLoss: 1.611411\n",
      "Train Epoch: 0 [10080/50000 (20%)]\tLoss: 1.568794\n",
      "Train Epoch: 0 [10160/50000 (20%)]\tLoss: 1.463617\n",
      "Train Epoch: 0 [10240/50000 (20%)]\tLoss: 2.218609\n",
      "Train Epoch: 0 [10320/50000 (21%)]\tLoss: 1.759878\n",
      "Train Epoch: 0 [10400/50000 (21%)]\tLoss: 2.047293\n",
      "Train Epoch: 0 [10480/50000 (21%)]\tLoss: 2.416391\n",
      "Train Epoch: 0 [10560/50000 (21%)]\tLoss: 2.127542\n",
      "Train Epoch: 0 [10640/50000 (21%)]\tLoss: 0.969473\n",
      "Train Epoch: 0 [10720/50000 (21%)]\tLoss: 0.732322\n",
      "Train Epoch: 0 [10800/50000 (22%)]\tLoss: 1.504930\n",
      "Train Epoch: 0 [10880/50000 (22%)]\tLoss: 1.204748\n",
      "Train Epoch: 0 [10960/50000 (22%)]\tLoss: 1.164693\n",
      "Train Epoch: 0 [11040/50000 (22%)]\tLoss: 1.791893\n",
      "Train Epoch: 0 [11120/50000 (22%)]\tLoss: 1.545690\n",
      "Train Epoch: 0 [11200/50000 (22%)]\tLoss: 1.347935\n",
      "Train Epoch: 0 [11280/50000 (23%)]\tLoss: 0.951580\n",
      "Train Epoch: 0 [11360/50000 (23%)]\tLoss: 1.225485\n",
      "Train Epoch: 0 [11440/50000 (23%)]\tLoss: 1.403852\n",
      "Train Epoch: 0 [11520/50000 (23%)]\tLoss: 1.650325\n",
      "Train Epoch: 0 [11600/50000 (23%)]\tLoss: 1.690081\n",
      "Train Epoch: 0 [11680/50000 (23%)]\tLoss: 0.915325\n",
      "Train Epoch: 0 [11760/50000 (24%)]\tLoss: 1.045286\n",
      "Train Epoch: 0 [11840/50000 (24%)]\tLoss: 0.917298\n",
      "Train Epoch: 0 [11920/50000 (24%)]\tLoss: 1.573241\n",
      "Train Epoch: 0 [12000/50000 (24%)]\tLoss: 1.617938\n",
      "Train Epoch: 0 [12080/50000 (24%)]\tLoss: 1.278114\n",
      "Train Epoch: 0 [12160/50000 (24%)]\tLoss: 1.474361\n",
      "Train Epoch: 0 [12240/50000 (24%)]\tLoss: 1.329271\n",
      "Train Epoch: 0 [12320/50000 (25%)]\tLoss: 1.691332\n",
      "Train Epoch: 0 [12400/50000 (25%)]\tLoss: 0.981218\n",
      "Train Epoch: 0 [12480/50000 (25%)]\tLoss: 1.565816\n",
      "Train Epoch: 0 [12560/50000 (25%)]\tLoss: 1.196336\n",
      "Train Epoch: 0 [12640/50000 (25%)]\tLoss: 0.978708\n",
      "Train Epoch: 0 [12720/50000 (25%)]\tLoss: 2.319250\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 1.498610\n",
      "Train Epoch: 0 [12880/50000 (26%)]\tLoss: 1.141728\n",
      "Train Epoch: 0 [12960/50000 (26%)]\tLoss: 1.675475\n",
      "Train Epoch: 0 [13040/50000 (26%)]\tLoss: 2.243598\n",
      "Train Epoch: 0 [13120/50000 (26%)]\tLoss: 1.161193\n",
      "Train Epoch: 0 [13200/50000 (26%)]\tLoss: 0.963197\n",
      "Train Epoch: 0 [13280/50000 (27%)]\tLoss: 1.301727\n",
      "Train Epoch: 0 [13360/50000 (27%)]\tLoss: 0.757545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [13440/50000 (27%)]\tLoss: 1.662176\n",
      "Train Epoch: 0 [13520/50000 (27%)]\tLoss: 1.203042\n",
      "Train Epoch: 0 [13600/50000 (27%)]\tLoss: 1.828025\n",
      "Train Epoch: 0 [13680/50000 (27%)]\tLoss: 1.677562\n",
      "Train Epoch: 0 [13760/50000 (28%)]\tLoss: 1.141060\n",
      "Train Epoch: 0 [13840/50000 (28%)]\tLoss: 1.526646\n",
      "Train Epoch: 0 [13920/50000 (28%)]\tLoss: 1.440589\n",
      "Train Epoch: 0 [14000/50000 (28%)]\tLoss: 1.554935\n",
      "Train Epoch: 0 [14080/50000 (28%)]\tLoss: 0.997076\n",
      "Train Epoch: 0 [14160/50000 (28%)]\tLoss: 1.295561\n",
      "Train Epoch: 0 [14240/50000 (28%)]\tLoss: 1.414037\n",
      "Train Epoch: 0 [14320/50000 (29%)]\tLoss: 1.393744\n",
      "Train Epoch: 0 [14400/50000 (29%)]\tLoss: 0.913512\n",
      "Train Epoch: 0 [14480/50000 (29%)]\tLoss: 0.775570\n",
      "Train Epoch: 0 [14560/50000 (29%)]\tLoss: 0.955433\n",
      "Train Epoch: 0 [14640/50000 (29%)]\tLoss: 1.952978\n",
      "Train Epoch: 0 [14720/50000 (29%)]\tLoss: 1.545125\n",
      "Train Epoch: 0 [14800/50000 (30%)]\tLoss: 0.940265\n",
      "Train Epoch: 0 [14880/50000 (30%)]\tLoss: 1.755618\n",
      "Train Epoch: 0 [14960/50000 (30%)]\tLoss: 1.169494\n",
      "Train Epoch: 0 [15040/50000 (30%)]\tLoss: 1.176231\n",
      "Train Epoch: 0 [15120/50000 (30%)]\tLoss: 1.814696\n",
      "Train Epoch: 0 [15200/50000 (30%)]\tLoss: 1.152095\n",
      "Train Epoch: 0 [15280/50000 (31%)]\tLoss: 1.672485\n",
      "Train Epoch: 0 [15360/50000 (31%)]\tLoss: 1.040197\n",
      "Train Epoch: 0 [15440/50000 (31%)]\tLoss: 1.110303\n",
      "Train Epoch: 0 [15520/50000 (31%)]\tLoss: 1.079257\n",
      "Train Epoch: 0 [15600/50000 (31%)]\tLoss: 1.646922\n",
      "Train Epoch: 0 [15680/50000 (31%)]\tLoss: 0.949824\n",
      "Train Epoch: 0 [15760/50000 (32%)]\tLoss: 1.010921\n",
      "Train Epoch: 0 [15840/50000 (32%)]\tLoss: 1.293572\n",
      "Train Epoch: 0 [15920/50000 (32%)]\tLoss: 0.926449\n",
      "Train Epoch: 0 [16000/50000 (32%)]\tLoss: 1.969348\n",
      "Train Epoch: 0 [16080/50000 (32%)]\tLoss: 0.726457\n",
      "Train Epoch: 0 [16160/50000 (32%)]\tLoss: 2.226806\n",
      "Train Epoch: 0 [16240/50000 (32%)]\tLoss: 1.481546\n",
      "Train Epoch: 0 [16320/50000 (33%)]\tLoss: 1.135480\n",
      "Train Epoch: 0 [16400/50000 (33%)]\tLoss: 1.168003\n",
      "Train Epoch: 0 [16480/50000 (33%)]\tLoss: 1.384084\n",
      "Train Epoch: 0 [16560/50000 (33%)]\tLoss: 1.093916\n",
      "Train Epoch: 0 [16640/50000 (33%)]\tLoss: 0.923677\n",
      "Train Epoch: 0 [16720/50000 (33%)]\tLoss: 1.399816\n",
      "Train Epoch: 0 [16800/50000 (34%)]\tLoss: 1.069088\n",
      "Train Epoch: 0 [16880/50000 (34%)]\tLoss: 0.918982\n",
      "Train Epoch: 0 [16960/50000 (34%)]\tLoss: 1.464326\n",
      "Train Epoch: 0 [17040/50000 (34%)]\tLoss: 1.633371\n",
      "Train Epoch: 0 [17120/50000 (34%)]\tLoss: 1.378457\n",
      "Train Epoch: 0 [17200/50000 (34%)]\tLoss: 1.598406\n",
      "Train Epoch: 0 [17280/50000 (35%)]\tLoss: 1.153124\n",
      "Train Epoch: 0 [17360/50000 (35%)]\tLoss: 0.992213\n",
      "Train Epoch: 0 [17440/50000 (35%)]\tLoss: 0.721450\n",
      "Train Epoch: 0 [17520/50000 (35%)]\tLoss: 1.587634\n",
      "Train Epoch: 0 [17600/50000 (35%)]\tLoss: 0.472987\n",
      "Train Epoch: 0 [17680/50000 (35%)]\tLoss: 0.784142\n",
      "Train Epoch: 0 [17760/50000 (36%)]\tLoss: 1.356275\n",
      "Train Epoch: 0 [17840/50000 (36%)]\tLoss: 1.914368\n",
      "Train Epoch: 0 [17920/50000 (36%)]\tLoss: 1.062446\n",
      "Train Epoch: 0 [18000/50000 (36%)]\tLoss: 1.506304\n",
      "Train Epoch: 0 [18080/50000 (36%)]\tLoss: 1.968541\n",
      "Train Epoch: 0 [18160/50000 (36%)]\tLoss: 1.185270\n",
      "Train Epoch: 0 [18240/50000 (36%)]\tLoss: 1.110387\n",
      "Train Epoch: 0 [18320/50000 (37%)]\tLoss: 1.237478\n",
      "Train Epoch: 0 [18400/50000 (37%)]\tLoss: 1.078550\n",
      "Train Epoch: 0 [18480/50000 (37%)]\tLoss: 1.434124\n",
      "Train Epoch: 0 [18560/50000 (37%)]\tLoss: 1.358972\n",
      "Train Epoch: 0 [18640/50000 (37%)]\tLoss: 0.892425\n",
      "Train Epoch: 0 [18720/50000 (37%)]\tLoss: 1.150492\n",
      "Train Epoch: 0 [18800/50000 (38%)]\tLoss: 1.024154\n",
      "Train Epoch: 0 [18880/50000 (38%)]\tLoss: 1.306627\n",
      "Train Epoch: 0 [18960/50000 (38%)]\tLoss: 1.039279\n",
      "Train Epoch: 0 [19040/50000 (38%)]\tLoss: 1.382236\n",
      "Train Epoch: 0 [19120/50000 (38%)]\tLoss: 1.583104\n",
      "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 1.353004\n",
      "Train Epoch: 0 [19280/50000 (39%)]\tLoss: 1.141169\n",
      "Train Epoch: 0 [19360/50000 (39%)]\tLoss: 1.357589\n",
      "Train Epoch: 0 [19440/50000 (39%)]\tLoss: 1.487578\n",
      "Train Epoch: 0 [19520/50000 (39%)]\tLoss: 1.451802\n",
      "Train Epoch: 0 [19600/50000 (39%)]\tLoss: 1.835285\n",
      "Train Epoch: 0 [19680/50000 (39%)]\tLoss: 1.371704\n",
      "Train Epoch: 0 [19760/50000 (40%)]\tLoss: 1.866935\n",
      "Train Epoch: 0 [19840/50000 (40%)]\tLoss: 1.062609\n",
      "Train Epoch: 0 [19920/50000 (40%)]\tLoss: 1.210630\n",
      "Train Epoch: 0 [20000/50000 (40%)]\tLoss: 1.813241\n",
      "Train Epoch: 0 [20080/50000 (40%)]\tLoss: 1.014699\n",
      "Train Epoch: 0 [20160/50000 (40%)]\tLoss: 0.972662\n",
      "Train Epoch: 0 [20240/50000 (40%)]\tLoss: 1.141602\n",
      "Train Epoch: 0 [20320/50000 (41%)]\tLoss: 1.235852\n",
      "Train Epoch: 0 [20400/50000 (41%)]\tLoss: 1.109417\n",
      "Train Epoch: 0 [20480/50000 (41%)]\tLoss: 1.513931\n",
      "Train Epoch: 0 [20560/50000 (41%)]\tLoss: 1.233041\n",
      "Train Epoch: 0 [20640/50000 (41%)]\tLoss: 1.416797\n",
      "Train Epoch: 0 [20720/50000 (41%)]\tLoss: 1.425227\n",
      "Train Epoch: 0 [20800/50000 (42%)]\tLoss: 1.123429\n",
      "Train Epoch: 0 [20880/50000 (42%)]\tLoss: 2.514721\n",
      "Train Epoch: 0 [20960/50000 (42%)]\tLoss: 1.629107\n",
      "Train Epoch: 0 [21040/50000 (42%)]\tLoss: 1.238790\n",
      "Train Epoch: 0 [21120/50000 (42%)]\tLoss: 0.417565\n",
      "Train Epoch: 0 [21200/50000 (42%)]\tLoss: 1.138854\n",
      "Train Epoch: 0 [21280/50000 (43%)]\tLoss: 1.481155\n",
      "Train Epoch: 0 [21360/50000 (43%)]\tLoss: 1.935257\n",
      "Train Epoch: 0 [21440/50000 (43%)]\tLoss: 0.815200\n",
      "Train Epoch: 0 [21520/50000 (43%)]\tLoss: 0.733993\n",
      "Train Epoch: 0 [21600/50000 (43%)]\tLoss: 0.760906\n",
      "Train Epoch: 0 [21680/50000 (43%)]\tLoss: 1.126229\n",
      "Train Epoch: 0 [21760/50000 (44%)]\tLoss: 1.088482\n",
      "Train Epoch: 0 [21840/50000 (44%)]\tLoss: 1.642820\n",
      "Train Epoch: 0 [21920/50000 (44%)]\tLoss: 1.347872\n",
      "Train Epoch: 0 [22000/50000 (44%)]\tLoss: 1.699397\n",
      "Train Epoch: 0 [22080/50000 (44%)]\tLoss: 1.463907\n",
      "Train Epoch: 0 [22160/50000 (44%)]\tLoss: 0.804256\n",
      "Train Epoch: 0 [22240/50000 (44%)]\tLoss: 1.668627\n",
      "Train Epoch: 0 [22320/50000 (45%)]\tLoss: 1.549860\n",
      "Train Epoch: 0 [22400/50000 (45%)]\tLoss: 0.901635\n",
      "Train Epoch: 0 [22480/50000 (45%)]\tLoss: 1.031095\n",
      "Train Epoch: 0 [22560/50000 (45%)]\tLoss: 1.512500\n",
      "Train Epoch: 0 [22640/50000 (45%)]\tLoss: 1.101495\n",
      "Train Epoch: 0 [22720/50000 (45%)]\tLoss: 0.912229\n",
      "Train Epoch: 0 [22800/50000 (46%)]\tLoss: 1.571868\n",
      "Train Epoch: 0 [22880/50000 (46%)]\tLoss: 1.415490\n",
      "Train Epoch: 0 [22960/50000 (46%)]\tLoss: 1.761779\n",
      "Train Epoch: 0 [23040/50000 (46%)]\tLoss: 0.877678\n",
      "Train Epoch: 0 [23120/50000 (46%)]\tLoss: 1.369992\n",
      "Train Epoch: 0 [23200/50000 (46%)]\tLoss: 1.170511\n",
      "Train Epoch: 0 [23280/50000 (47%)]\tLoss: 1.431675\n",
      "Train Epoch: 0 [23360/50000 (47%)]\tLoss: 0.929297\n",
      "Train Epoch: 0 [23440/50000 (47%)]\tLoss: 1.443360\n",
      "Train Epoch: 0 [23520/50000 (47%)]\tLoss: 0.718600\n",
      "Train Epoch: 0 [23600/50000 (47%)]\tLoss: 1.571036\n",
      "Train Epoch: 0 [23680/50000 (47%)]\tLoss: 1.714650\n",
      "Train Epoch: 0 [23760/50000 (48%)]\tLoss: 1.195657\n",
      "Train Epoch: 0 [23840/50000 (48%)]\tLoss: 0.953467\n",
      "Train Epoch: 0 [23920/50000 (48%)]\tLoss: 1.210131\n",
      "Train Epoch: 0 [24000/50000 (48%)]\tLoss: 1.808074\n",
      "Train Epoch: 0 [24080/50000 (48%)]\tLoss: 2.028216\n",
      "Train Epoch: 0 [24160/50000 (48%)]\tLoss: 1.736582\n",
      "Train Epoch: 0 [24240/50000 (48%)]\tLoss: 0.667110\n",
      "Train Epoch: 0 [24320/50000 (49%)]\tLoss: 1.506350\n",
      "Train Epoch: 0 [24400/50000 (49%)]\tLoss: 1.288172\n",
      "Train Epoch: 0 [24480/50000 (49%)]\tLoss: 1.056300\n",
      "Train Epoch: 0 [24560/50000 (49%)]\tLoss: 1.653667\n",
      "Train Epoch: 0 [24640/50000 (49%)]\tLoss: 1.847442\n",
      "Train Epoch: 0 [24720/50000 (49%)]\tLoss: 1.385773\n",
      "Train Epoch: 0 [24800/50000 (50%)]\tLoss: 1.374854\n",
      "Train Epoch: 0 [24880/50000 (50%)]\tLoss: 1.973179\n",
      "Train Epoch: 0 [24960/50000 (50%)]\tLoss: 1.624881\n",
      "Train Epoch: 0 [25040/50000 (50%)]\tLoss: 1.119236\n",
      "Train Epoch: 0 [25120/50000 (50%)]\tLoss: 1.563408\n",
      "Train Epoch: 0 [25200/50000 (50%)]\tLoss: 1.025915\n",
      "Train Epoch: 0 [25280/50000 (51%)]\tLoss: 0.742277\n",
      "Train Epoch: 0 [25360/50000 (51%)]\tLoss: 1.170145\n",
      "Train Epoch: 0 [25440/50000 (51%)]\tLoss: 0.999743\n",
      "Train Epoch: 0 [25520/50000 (51%)]\tLoss: 1.565990\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 1.512331\n",
      "Train Epoch: 0 [25680/50000 (51%)]\tLoss: 1.310527\n",
      "Train Epoch: 0 [25760/50000 (52%)]\tLoss: 1.071787\n",
      "Train Epoch: 0 [25840/50000 (52%)]\tLoss: 1.200142\n",
      "Train Epoch: 0 [25920/50000 (52%)]\tLoss: 1.150680\n",
      "Train Epoch: 0 [26000/50000 (52%)]\tLoss: 2.387289\n",
      "Train Epoch: 0 [26080/50000 (52%)]\tLoss: 0.997732\n",
      "Train Epoch: 0 [26160/50000 (52%)]\tLoss: 1.298515\n",
      "Train Epoch: 0 [26240/50000 (52%)]\tLoss: 1.171274\n",
      "Train Epoch: 0 [26320/50000 (53%)]\tLoss: 1.207327\n",
      "Train Epoch: 0 [26400/50000 (53%)]\tLoss: 1.189364\n",
      "Train Epoch: 0 [26480/50000 (53%)]\tLoss: 1.061330\n",
      "Train Epoch: 0 [26560/50000 (53%)]\tLoss: 0.929939\n",
      "Train Epoch: 0 [26640/50000 (53%)]\tLoss: 1.875299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [26720/50000 (53%)]\tLoss: 0.910874\n",
      "Train Epoch: 0 [26800/50000 (54%)]\tLoss: 1.429273\n",
      "Train Epoch: 0 [26880/50000 (54%)]\tLoss: 0.687381\n",
      "Train Epoch: 0 [26960/50000 (54%)]\tLoss: 1.041525\n",
      "Train Epoch: 0 [27040/50000 (54%)]\tLoss: 1.302322\n",
      "Train Epoch: 0 [27120/50000 (54%)]\tLoss: 1.106109\n",
      "Train Epoch: 0 [27200/50000 (54%)]\tLoss: 1.099314\n",
      "Train Epoch: 0 [27280/50000 (55%)]\tLoss: 1.687588\n",
      "Train Epoch: 0 [27360/50000 (55%)]\tLoss: 1.144522\n",
      "Train Epoch: 0 [27440/50000 (55%)]\tLoss: 1.478920\n",
      "Train Epoch: 0 [27520/50000 (55%)]\tLoss: 1.386644\n",
      "Train Epoch: 0 [27600/50000 (55%)]\tLoss: 1.743778\n",
      "Train Epoch: 0 [27680/50000 (55%)]\tLoss: 1.803428\n",
      "Train Epoch: 0 [27760/50000 (56%)]\tLoss: 1.496379\n",
      "Train Epoch: 0 [27840/50000 (56%)]\tLoss: 0.968785\n",
      "Train Epoch: 0 [27920/50000 (56%)]\tLoss: 1.731007\n",
      "Train Epoch: 0 [28000/50000 (56%)]\tLoss: 0.693947\n",
      "Train Epoch: 0 [28080/50000 (56%)]\tLoss: 1.548208\n",
      "Train Epoch: 0 [28160/50000 (56%)]\tLoss: 0.907927\n",
      "Train Epoch: 0 [28240/50000 (56%)]\tLoss: 0.843146\n",
      "Train Epoch: 0 [28320/50000 (57%)]\tLoss: 1.596612\n",
      "Train Epoch: 0 [28400/50000 (57%)]\tLoss: 1.562942\n",
      "Train Epoch: 0 [28480/50000 (57%)]\tLoss: 1.701987\n",
      "Train Epoch: 0 [28560/50000 (57%)]\tLoss: 1.521843\n",
      "Train Epoch: 0 [28640/50000 (57%)]\tLoss: 1.474183\n",
      "Train Epoch: 0 [28720/50000 (57%)]\tLoss: 1.104842\n",
      "Train Epoch: 0 [28800/50000 (58%)]\tLoss: 1.566114\n",
      "Train Epoch: 0 [28880/50000 (58%)]\tLoss: 2.449299\n",
      "Train Epoch: 0 [28960/50000 (58%)]\tLoss: 1.500472\n",
      "Train Epoch: 0 [29040/50000 (58%)]\tLoss: 1.211900\n",
      "Train Epoch: 0 [29120/50000 (58%)]\tLoss: 1.070981\n",
      "Train Epoch: 0 [29200/50000 (58%)]\tLoss: 1.070733\n",
      "Train Epoch: 0 [29280/50000 (59%)]\tLoss: 1.084015\n",
      "Train Epoch: 0 [29360/50000 (59%)]\tLoss: 1.560381\n",
      "Train Epoch: 0 [29440/50000 (59%)]\tLoss: 1.116270\n",
      "Train Epoch: 0 [29520/50000 (59%)]\tLoss: 1.756112\n",
      "Train Epoch: 0 [29600/50000 (59%)]\tLoss: 0.762604\n",
      "Train Epoch: 0 [29680/50000 (59%)]\tLoss: 1.304322\n",
      "Train Epoch: 0 [29760/50000 (60%)]\tLoss: 1.273971\n",
      "Train Epoch: 0 [29840/50000 (60%)]\tLoss: 1.491925\n",
      "Train Epoch: 0 [29920/50000 (60%)]\tLoss: 0.915163\n",
      "Train Epoch: 0 [30000/50000 (60%)]\tLoss: 2.256352\n",
      "Train Epoch: 0 [30080/50000 (60%)]\tLoss: 0.939336\n",
      "Train Epoch: 0 [30160/50000 (60%)]\tLoss: 0.465242\n",
      "Train Epoch: 0 [30240/50000 (60%)]\tLoss: 1.144240\n",
      "Train Epoch: 0 [30320/50000 (61%)]\tLoss: 0.806928\n",
      "Train Epoch: 0 [30400/50000 (61%)]\tLoss: 1.859298\n",
      "Train Epoch: 0 [30480/50000 (61%)]\tLoss: 1.350943\n",
      "Train Epoch: 0 [30560/50000 (61%)]\tLoss: 1.124684\n",
      "Train Epoch: 0 [30640/50000 (61%)]\tLoss: 1.316707\n",
      "Train Epoch: 0 [30720/50000 (61%)]\tLoss: 0.869056\n",
      "Train Epoch: 0 [30800/50000 (62%)]\tLoss: 0.594051\n",
      "Train Epoch: 0 [30880/50000 (62%)]\tLoss: 1.188751\n",
      "Train Epoch: 0 [30960/50000 (62%)]\tLoss: 1.123393\n",
      "Train Epoch: 0 [31040/50000 (62%)]\tLoss: 1.662598\n",
      "Train Epoch: 0 [31120/50000 (62%)]\tLoss: 0.983859\n",
      "Train Epoch: 0 [31200/50000 (62%)]\tLoss: 1.678684\n",
      "Train Epoch: 0 [31280/50000 (63%)]\tLoss: 0.568805\n",
      "Train Epoch: 0 [31360/50000 (63%)]\tLoss: 1.679210\n",
      "Train Epoch: 0 [31440/50000 (63%)]\tLoss: 2.528597\n",
      "Train Epoch: 0 [31520/50000 (63%)]\tLoss: 1.098963\n",
      "Train Epoch: 0 [31600/50000 (63%)]\tLoss: 1.749222\n",
      "Train Epoch: 0 [31680/50000 (63%)]\tLoss: 1.405198\n",
      "Train Epoch: 0 [31760/50000 (64%)]\tLoss: 1.528574\n",
      "Train Epoch: 0 [31840/50000 (64%)]\tLoss: 1.066686\n",
      "Train Epoch: 0 [31920/50000 (64%)]\tLoss: 1.743450\n",
      "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 1.144149\n",
      "Train Epoch: 0 [32080/50000 (64%)]\tLoss: 0.701241\n",
      "Train Epoch: 0 [32160/50000 (64%)]\tLoss: 0.965515\n",
      "Train Epoch: 0 [32240/50000 (64%)]\tLoss: 0.427181\n",
      "Train Epoch: 0 [32320/50000 (65%)]\tLoss: 1.055521\n",
      "Train Epoch: 0 [32400/50000 (65%)]\tLoss: 1.618826\n",
      "Train Epoch: 0 [32480/50000 (65%)]\tLoss: 2.067329\n",
      "Train Epoch: 0 [32560/50000 (65%)]\tLoss: 0.726335\n",
      "Train Epoch: 0 [32640/50000 (65%)]\tLoss: 0.744424\n",
      "Train Epoch: 0 [32720/50000 (65%)]\tLoss: 1.231115\n",
      "Train Epoch: 0 [32800/50000 (66%)]\tLoss: 1.283793\n",
      "Train Epoch: 0 [32880/50000 (66%)]\tLoss: 1.066308\n",
      "Train Epoch: 0 [32960/50000 (66%)]\tLoss: 2.344922\n",
      "Train Epoch: 0 [33040/50000 (66%)]\tLoss: 1.049684\n",
      "Train Epoch: 0 [33120/50000 (66%)]\tLoss: 0.896536\n",
      "Train Epoch: 0 [33200/50000 (66%)]\tLoss: 0.982401\n",
      "Train Epoch: 0 [33280/50000 (67%)]\tLoss: 1.731709\n",
      "Train Epoch: 0 [33360/50000 (67%)]\tLoss: 1.130465\n",
      "Train Epoch: 0 [33440/50000 (67%)]\tLoss: 1.144457\n",
      "Train Epoch: 0 [33520/50000 (67%)]\tLoss: 1.283564\n",
      "Train Epoch: 0 [33600/50000 (67%)]\tLoss: 2.289505\n",
      "Train Epoch: 0 [33680/50000 (67%)]\tLoss: 1.641308\n",
      "Train Epoch: 0 [33760/50000 (68%)]\tLoss: 1.794374\n",
      "Train Epoch: 0 [33840/50000 (68%)]\tLoss: 1.023349\n",
      "Train Epoch: 0 [33920/50000 (68%)]\tLoss: 1.840860\n",
      "Train Epoch: 0 [34000/50000 (68%)]\tLoss: 1.076368\n",
      "Train Epoch: 0 [34080/50000 (68%)]\tLoss: 1.321919\n",
      "Train Epoch: 0 [34160/50000 (68%)]\tLoss: 0.953951\n",
      "Train Epoch: 0 [34240/50000 (68%)]\tLoss: 1.877535\n",
      "Train Epoch: 0 [34320/50000 (69%)]\tLoss: 1.437125\n",
      "Train Epoch: 0 [34400/50000 (69%)]\tLoss: 1.658853\n",
      "Train Epoch: 0 [34480/50000 (69%)]\tLoss: 1.385854\n",
      "Train Epoch: 0 [34560/50000 (69%)]\tLoss: 1.493677\n",
      "Train Epoch: 0 [34640/50000 (69%)]\tLoss: 0.877010\n",
      "Train Epoch: 0 [34720/50000 (69%)]\tLoss: 0.880714\n",
      "Train Epoch: 0 [34800/50000 (70%)]\tLoss: 1.615793\n",
      "Train Epoch: 0 [34880/50000 (70%)]\tLoss: 1.262882\n",
      "Train Epoch: 0 [34960/50000 (70%)]\tLoss: 1.322669\n",
      "Train Epoch: 0 [35040/50000 (70%)]\tLoss: 1.108953\n",
      "Train Epoch: 0 [35120/50000 (70%)]\tLoss: 1.608006\n",
      "Train Epoch: 0 [35200/50000 (70%)]\tLoss: 1.038926\n",
      "Train Epoch: 0 [35280/50000 (71%)]\tLoss: 0.731501\n",
      "Train Epoch: 0 [35360/50000 (71%)]\tLoss: 0.740925\n",
      "Train Epoch: 0 [35440/50000 (71%)]\tLoss: 0.893468\n",
      "Train Epoch: 0 [35520/50000 (71%)]\tLoss: 1.446715\n",
      "Train Epoch: 0 [35600/50000 (71%)]\tLoss: 0.727850\n",
      "Train Epoch: 0 [35680/50000 (71%)]\tLoss: 1.695612\n",
      "Train Epoch: 0 [35760/50000 (72%)]\tLoss: 1.458888\n",
      "Train Epoch: 0 [35840/50000 (72%)]\tLoss: 1.601017\n",
      "Train Epoch: 0 [35920/50000 (72%)]\tLoss: 1.692999\n",
      "Train Epoch: 0 [36000/50000 (72%)]\tLoss: 0.890847\n",
      "Train Epoch: 0 [36080/50000 (72%)]\tLoss: 2.122311\n",
      "Train Epoch: 0 [36160/50000 (72%)]\tLoss: 1.633210\n",
      "Train Epoch: 0 [36240/50000 (72%)]\tLoss: 1.196306\n",
      "Train Epoch: 0 [36320/50000 (73%)]\tLoss: 1.083451\n",
      "Train Epoch: 0 [36400/50000 (73%)]\tLoss: 0.642083\n",
      "Train Epoch: 0 [36480/50000 (73%)]\tLoss: 1.815020\n",
      "Train Epoch: 0 [36560/50000 (73%)]\tLoss: 1.024517\n",
      "Train Epoch: 0 [36640/50000 (73%)]\tLoss: 0.868728\n",
      "Train Epoch: 0 [36720/50000 (73%)]\tLoss: 1.260405\n",
      "Train Epoch: 0 [36800/50000 (74%)]\tLoss: 1.255955\n",
      "Train Epoch: 0 [36880/50000 (74%)]\tLoss: 1.626587\n",
      "Train Epoch: 0 [36960/50000 (74%)]\tLoss: 1.490483\n",
      "Train Epoch: 0 [37040/50000 (74%)]\tLoss: 1.746628\n",
      "Train Epoch: 0 [37120/50000 (74%)]\tLoss: 1.880161\n",
      "Train Epoch: 0 [37200/50000 (74%)]\tLoss: 1.178917\n",
      "Train Epoch: 0 [37280/50000 (75%)]\tLoss: 0.982867\n",
      "Train Epoch: 0 [37360/50000 (75%)]\tLoss: 1.214158\n",
      "Train Epoch: 0 [37440/50000 (75%)]\tLoss: 1.264241\n",
      "Train Epoch: 0 [37520/50000 (75%)]\tLoss: 0.925264\n",
      "Train Epoch: 0 [37600/50000 (75%)]\tLoss: 1.738583\n",
      "Train Epoch: 0 [37680/50000 (75%)]\tLoss: 1.130196\n",
      "Train Epoch: 0 [37760/50000 (76%)]\tLoss: 1.958157\n",
      "Train Epoch: 0 [37840/50000 (76%)]\tLoss: 0.841873\n",
      "Train Epoch: 0 [37920/50000 (76%)]\tLoss: 1.407453\n",
      "Train Epoch: 0 [38000/50000 (76%)]\tLoss: 1.384117\n",
      "Train Epoch: 0 [38080/50000 (76%)]\tLoss: 0.900709\n",
      "Train Epoch: 0 [38160/50000 (76%)]\tLoss: 2.101379\n",
      "Train Epoch: 0 [38240/50000 (76%)]\tLoss: 1.537924\n",
      "Train Epoch: 0 [38320/50000 (77%)]\tLoss: 1.699661\n",
      "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 0.378412\n",
      "Train Epoch: 0 [38480/50000 (77%)]\tLoss: 2.378830\n",
      "Train Epoch: 0 [38560/50000 (77%)]\tLoss: 0.949149\n",
      "Train Epoch: 0 [38640/50000 (77%)]\tLoss: 1.204718\n",
      "Train Epoch: 0 [38720/50000 (77%)]\tLoss: 1.192309\n",
      "Train Epoch: 0 [38800/50000 (78%)]\tLoss: 1.088654\n",
      "Train Epoch: 0 [38880/50000 (78%)]\tLoss: 2.060083\n",
      "Train Epoch: 0 [38960/50000 (78%)]\tLoss: 1.238334\n",
      "Train Epoch: 0 [39040/50000 (78%)]\tLoss: 0.907322\n",
      "Train Epoch: 0 [39120/50000 (78%)]\tLoss: 1.311159\n",
      "Train Epoch: 0 [39200/50000 (78%)]\tLoss: 1.184041\n",
      "Train Epoch: 0 [39280/50000 (79%)]\tLoss: 1.433157\n",
      "Train Epoch: 0 [39360/50000 (79%)]\tLoss: 1.031669\n",
      "Train Epoch: 0 [39440/50000 (79%)]\tLoss: 1.314984\n",
      "Train Epoch: 0 [39520/50000 (79%)]\tLoss: 1.375714\n",
      "Train Epoch: 0 [39600/50000 (79%)]\tLoss: 1.774799\n",
      "Train Epoch: 0 [39680/50000 (79%)]\tLoss: 1.344297\n",
      "Train Epoch: 0 [39760/50000 (80%)]\tLoss: 1.179338\n",
      "Train Epoch: 0 [39840/50000 (80%)]\tLoss: 1.408562\n",
      "Train Epoch: 0 [39920/50000 (80%)]\tLoss: 0.987526\n",
      "Train Epoch: 0 [40000/50000 (80%)]\tLoss: 1.391856\n",
      "Train Epoch: 0 [40080/50000 (80%)]\tLoss: 1.003216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [40160/50000 (80%)]\tLoss: 1.068597\n",
      "Train Epoch: 0 [40240/50000 (80%)]\tLoss: 1.136812\n",
      "Train Epoch: 0 [40320/50000 (81%)]\tLoss: 1.038379\n",
      "Train Epoch: 0 [40400/50000 (81%)]\tLoss: 1.205004\n",
      "Train Epoch: 0 [40480/50000 (81%)]\tLoss: 1.187584\n",
      "Train Epoch: 0 [40560/50000 (81%)]\tLoss: 1.021250\n",
      "Train Epoch: 0 [40640/50000 (81%)]\tLoss: 1.022117\n",
      "Train Epoch: 0 [40720/50000 (81%)]\tLoss: 1.299683\n",
      "Train Epoch: 0 [40800/50000 (82%)]\tLoss: 1.135348\n",
      "Train Epoch: 0 [40880/50000 (82%)]\tLoss: 1.598607\n",
      "Train Epoch: 0 [40960/50000 (82%)]\tLoss: 1.536172\n",
      "Train Epoch: 0 [41040/50000 (82%)]\tLoss: 0.790663\n",
      "Train Epoch: 0 [41120/50000 (82%)]\tLoss: 0.931056\n",
      "Train Epoch: 0 [41200/50000 (82%)]\tLoss: 1.237830\n",
      "Train Epoch: 0 [41280/50000 (83%)]\tLoss: 1.044434\n",
      "Train Epoch: 0 [41360/50000 (83%)]\tLoss: 1.622200\n",
      "Train Epoch: 0 [41440/50000 (83%)]\tLoss: 1.248175\n",
      "Train Epoch: 0 [41520/50000 (83%)]\tLoss: 1.907423\n",
      "Train Epoch: 0 [41600/50000 (83%)]\tLoss: 1.319073\n",
      "Train Epoch: 0 [41680/50000 (83%)]\tLoss: 2.047173\n",
      "Train Epoch: 0 [41760/50000 (84%)]\tLoss: 1.478672\n",
      "Train Epoch: 0 [41840/50000 (84%)]\tLoss: 2.762529\n",
      "Train Epoch: 0 [41920/50000 (84%)]\tLoss: 1.451285\n",
      "Train Epoch: 0 [42000/50000 (84%)]\tLoss: 1.148488\n",
      "Train Epoch: 0 [42080/50000 (84%)]\tLoss: 1.589532\n",
      "Train Epoch: 0 [42160/50000 (84%)]\tLoss: 1.349283\n",
      "Train Epoch: 0 [42240/50000 (84%)]\tLoss: 0.409310\n",
      "Train Epoch: 0 [42320/50000 (85%)]\tLoss: 1.552313\n",
      "Train Epoch: 0 [42400/50000 (85%)]\tLoss: 1.079946\n",
      "Train Epoch: 0 [42480/50000 (85%)]\tLoss: 1.545966\n",
      "Train Epoch: 0 [42560/50000 (85%)]\tLoss: 1.374343\n",
      "Train Epoch: 0 [42640/50000 (85%)]\tLoss: 0.698987\n",
      "Train Epoch: 0 [42720/50000 (85%)]\tLoss: 1.763929\n",
      "Train Epoch: 0 [42800/50000 (86%)]\tLoss: 1.460678\n",
      "Train Epoch: 0 [42880/50000 (86%)]\tLoss: 1.125317\n",
      "Train Epoch: 0 [42960/50000 (86%)]\tLoss: 0.988665\n",
      "Train Epoch: 0 [43040/50000 (86%)]\tLoss: 0.983435\n",
      "Train Epoch: 0 [43120/50000 (86%)]\tLoss: 1.262049\n",
      "Train Epoch: 0 [43200/50000 (86%)]\tLoss: 1.562278\n",
      "Train Epoch: 0 [43280/50000 (87%)]\tLoss: 1.914154\n",
      "Train Epoch: 0 [43360/50000 (87%)]\tLoss: 0.757667\n",
      "Train Epoch: 0 [43440/50000 (87%)]\tLoss: 1.402836\n",
      "Train Epoch: 0 [43520/50000 (87%)]\tLoss: 1.006447\n",
      "Train Epoch: 0 [43600/50000 (87%)]\tLoss: 1.148288\n",
      "Train Epoch: 0 [43680/50000 (87%)]\tLoss: 0.804227\n",
      "Train Epoch: 0 [43760/50000 (88%)]\tLoss: 1.737994\n",
      "Train Epoch: 0 [43840/50000 (88%)]\tLoss: 0.940731\n",
      "Train Epoch: 0 [43920/50000 (88%)]\tLoss: 1.391297\n",
      "Train Epoch: 0 [44000/50000 (88%)]\tLoss: 0.659169\n",
      "Train Epoch: 0 [44080/50000 (88%)]\tLoss: 1.685101\n",
      "Train Epoch: 0 [44160/50000 (88%)]\tLoss: 1.242124\n",
      "Train Epoch: 0 [44240/50000 (88%)]\tLoss: 2.328106\n",
      "Train Epoch: 0 [44320/50000 (89%)]\tLoss: 0.871049\n",
      "Train Epoch: 0 [44400/50000 (89%)]\tLoss: 1.142930\n",
      "Train Epoch: 0 [44480/50000 (89%)]\tLoss: 1.863570\n",
      "Train Epoch: 0 [44560/50000 (89%)]\tLoss: 2.260263\n",
      "Train Epoch: 0 [44640/50000 (89%)]\tLoss: 1.093679\n",
      "Train Epoch: 0 [44720/50000 (89%)]\tLoss: 1.511732\n",
      "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 1.359360\n",
      "Train Epoch: 0 [44880/50000 (90%)]\tLoss: 1.047884\n",
      "Train Epoch: 0 [44960/50000 (90%)]\tLoss: 1.038116\n",
      "Train Epoch: 0 [45040/50000 (90%)]\tLoss: 1.280351\n",
      "Train Epoch: 0 [45120/50000 (90%)]\tLoss: 2.309371\n",
      "Train Epoch: 0 [45200/50000 (90%)]\tLoss: 1.069132\n",
      "Train Epoch: 0 [45280/50000 (91%)]\tLoss: 0.733293\n",
      "Train Epoch: 0 [45360/50000 (91%)]\tLoss: 0.875144\n",
      "Train Epoch: 0 [45440/50000 (91%)]\tLoss: 2.027849\n",
      "Train Epoch: 0 [45520/50000 (91%)]\tLoss: 1.137711\n",
      "Train Epoch: 0 [45600/50000 (91%)]\tLoss: 0.433117\n",
      "Train Epoch: 0 [45680/50000 (91%)]\tLoss: 0.821578\n",
      "Train Epoch: 0 [45760/50000 (92%)]\tLoss: 0.655248\n",
      "Train Epoch: 0 [45840/50000 (92%)]\tLoss: 0.949901\n",
      "Train Epoch: 0 [45920/50000 (92%)]\tLoss: 1.276950\n",
      "Train Epoch: 0 [46000/50000 (92%)]\tLoss: 0.824055\n",
      "Train Epoch: 0 [46080/50000 (92%)]\tLoss: 0.898967\n",
      "Train Epoch: 0 [46160/50000 (92%)]\tLoss: 1.283605\n",
      "Train Epoch: 0 [46240/50000 (92%)]\tLoss: 1.315191\n",
      "Train Epoch: 0 [46320/50000 (93%)]\tLoss: 0.873876\n",
      "Train Epoch: 0 [46400/50000 (93%)]\tLoss: 0.998574\n",
      "Train Epoch: 0 [46480/50000 (93%)]\tLoss: 1.052813\n",
      "Train Epoch: 0 [46560/50000 (93%)]\tLoss: 1.727360\n",
      "Train Epoch: 0 [46640/50000 (93%)]\tLoss: 0.657526\n",
      "Train Epoch: 0 [46720/50000 (93%)]\tLoss: 1.497878\n",
      "Train Epoch: 0 [46800/50000 (94%)]\tLoss: 0.976442\n",
      "Train Epoch: 0 [46880/50000 (94%)]\tLoss: 1.299559\n",
      "Train Epoch: 0 [46960/50000 (94%)]\tLoss: 1.152562\n",
      "Train Epoch: 0 [47040/50000 (94%)]\tLoss: 0.615273\n",
      "Train Epoch: 0 [47120/50000 (94%)]\tLoss: 1.607581\n",
      "Train Epoch: 0 [47200/50000 (94%)]\tLoss: 1.142216\n",
      "Train Epoch: 0 [47280/50000 (95%)]\tLoss: 1.158335\n",
      "Train Epoch: 0 [47360/50000 (95%)]\tLoss: 1.549828\n",
      "Train Epoch: 0 [47440/50000 (95%)]\tLoss: 1.193056\n",
      "Train Epoch: 0 [47520/50000 (95%)]\tLoss: 1.040726\n",
      "Train Epoch: 0 [47600/50000 (95%)]\tLoss: 1.707879\n",
      "Train Epoch: 0 [47680/50000 (95%)]\tLoss: 1.125646\n",
      "Train Epoch: 0 [47760/50000 (96%)]\tLoss: 1.529470\n",
      "Train Epoch: 0 [47840/50000 (96%)]\tLoss: 1.331152\n",
      "Train Epoch: 0 [47920/50000 (96%)]\tLoss: 0.865876\n",
      "Train Epoch: 0 [48000/50000 (96%)]\tLoss: 1.185072\n",
      "Train Epoch: 0 [48080/50000 (96%)]\tLoss: 0.574771\n",
      "Train Epoch: 0 [48160/50000 (96%)]\tLoss: 0.901322\n",
      "Train Epoch: 0 [48240/50000 (96%)]\tLoss: 1.752684\n",
      "Train Epoch: 0 [48320/50000 (97%)]\tLoss: 0.473437\n",
      "Train Epoch: 0 [48400/50000 (97%)]\tLoss: 0.907974\n",
      "Train Epoch: 0 [48480/50000 (97%)]\tLoss: 1.090418\n",
      "Train Epoch: 0 [48560/50000 (97%)]\tLoss: 1.042897\n",
      "Train Epoch: 0 [48640/50000 (97%)]\tLoss: 0.968843\n",
      "Train Epoch: 0 [48720/50000 (97%)]\tLoss: 0.606616\n",
      "Train Epoch: 0 [48800/50000 (98%)]\tLoss: 0.674523\n",
      "Train Epoch: 0 [48880/50000 (98%)]\tLoss: 0.783928\n",
      "Train Epoch: 0 [48960/50000 (98%)]\tLoss: 1.203350\n",
      "Train Epoch: 0 [49040/50000 (98%)]\tLoss: 2.474174\n",
      "Train Epoch: 0 [49120/50000 (98%)]\tLoss: 1.678526\n",
      "Train Epoch: 0 [49200/50000 (98%)]\tLoss: 0.714655\n",
      "Train Epoch: 0 [49280/50000 (99%)]\tLoss: 1.179405\n",
      "Train Epoch: 0 [49360/50000 (99%)]\tLoss: 1.719036\n",
      "Train Epoch: 0 [49440/50000 (99%)]\tLoss: 1.235975\n",
      "Train Epoch: 0 [49520/50000 (99%)]\tLoss: 1.702660\n",
      "Train Epoch: 0 [49600/50000 (99%)]\tLoss: 0.543430\n",
      "Train Epoch: 0 [49680/50000 (99%)]\tLoss: 1.333043\n",
      "Train Epoch: 0 [49760/50000 (100%)]\tLoss: 1.578026\n",
      "Train Epoch: 0 [49840/50000 (100%)]\tLoss: 0.970850\n",
      "Train Epoch: 0 [49920/50000 (100%)]\tLoss: 1.220155\n",
      "\n",
      "Test set: Average loss: 1.1928, Accuracy: 5764/10000 (58%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5764,\n",
       " OrderedDict([('conv1.weight',\n",
       "               tensor([[[[-0.1792, -0.3007, -0.1634,  0.1001, -0.0307],\n",
       "                         [-0.0394, -0.0261,  0.1543,  0.1128, -0.0070],\n",
       "                         [ 0.0913,  0.1881,  0.1692, -0.0329, -0.0872],\n",
       "                         [ 0.1587,  0.2333,  0.0790,  0.0762,  0.0477],\n",
       "                         [ 0.1308,  0.0547, -0.0104, -0.0324,  0.1052]],\n",
       "               \n",
       "                        [[-0.0994, -0.1031, -0.0246, -0.0360,  0.1676],\n",
       "                         [-0.0376,  0.0994,  0.1251, -0.0402, -0.1579],\n",
       "                         [-0.0198,  0.2302,  0.0558, -0.0179, -0.2188],\n",
       "                         [ 0.2220,  0.0784,  0.0726, -0.1091, -0.1700],\n",
       "                         [ 0.0931, -0.0725, -0.0874, -0.1331, -0.0165]],\n",
       "               \n",
       "                        [[-0.1261, -0.0620,  0.1632,  0.1851,  0.2982],\n",
       "                         [ 0.0472,  0.0953,  0.1070,  0.2150,  0.0227],\n",
       "                         [-0.0247,  0.2783,  0.2085,  0.0753, -0.2396],\n",
       "                         [ 0.1047,  0.1824,  0.0592, -0.1661, -0.1983],\n",
       "                         [ 0.0786, -0.0149, -0.0312, -0.1081,  0.0026]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0241, -0.0218, -0.0363, -0.2251, -0.0510],\n",
       "                         [-0.2259, -0.2704, -0.2311, -0.2155, -0.1508],\n",
       "                         [-0.2789, -0.3153, -0.1613, -0.1533, -0.1063],\n",
       "                         [-0.1850, -0.2895, -0.1585, -0.1103, -0.1930],\n",
       "                         [-0.1955, -0.1360, -0.1075, -0.1484, -0.1895]],\n",
       "               \n",
       "                        [[ 0.0523, -0.0360,  0.0553,  0.0064, -0.1707],\n",
       "                         [-0.0752,  0.1601,  0.0269,  0.1119, -0.0613],\n",
       "                         [ 0.0530,  0.1383,  0.1645,  0.0158, -0.0036],\n",
       "                         [ 0.0666,  0.1032,  0.1085, -0.0260, -0.1032],\n",
       "                         [-0.1451, -0.0481,  0.0868, -0.0855, -0.0777]],\n",
       "               \n",
       "                        [[-0.0013,  0.0406,  0.1508,  0.1998,  0.1152],\n",
       "                         [ 0.0835,  0.2386,  0.3120,  0.2752,  0.1161],\n",
       "                         [ 0.2217,  0.3212,  0.2610,  0.2019,  0.0283],\n",
       "                         [ 0.0493,  0.2064,  0.1538,  0.2541,  0.1646],\n",
       "                         [ 0.0397,  0.1440,  0.1910,  0.2167,  0.0776]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0397,  0.3717,  0.3042,  0.1806, -0.0374],\n",
       "                         [-0.2808, -0.3653, -0.3558, -0.1444, -0.1863],\n",
       "                         [-0.4337, -0.5040, -0.5084, -0.3071, -0.2549],\n",
       "                         [ 0.1445,  0.0872,  0.1642,  0.0218,  0.0451],\n",
       "                         [ 0.2909,  0.4152,  0.4696,  0.4178,  0.3455]],\n",
       "               \n",
       "                        [[ 0.2196,  0.3905,  0.3782,  0.1447,  0.2245],\n",
       "                         [-0.0980, -0.1616, -0.2625, -0.0995, -0.1013],\n",
       "                         [-0.1671, -0.5526, -0.5334, -0.2685, -0.1811],\n",
       "                         [ 0.0475,  0.0375,  0.0990,  0.0918,  0.0844],\n",
       "                         [-0.0790,  0.1758,  0.3142,  0.2478, -0.2016]],\n",
       "               \n",
       "                        [[ 0.1160,  0.1695,  0.3954,  0.1234,  0.1528],\n",
       "                         [ 0.0371, -0.0463, -0.0821,  0.0065,  0.0471],\n",
       "                         [-0.1325, -0.2925, -0.4669, -0.1696, -0.0419],\n",
       "                         [ 0.1905,  0.1800,  0.0448, -0.0066,  0.0361],\n",
       "                         [-0.0557,  0.1869,  0.0630, -0.0228, -0.1152]]],\n",
       "               \n",
       "               \n",
       "                       [[[-0.0509, -0.0554, -0.1962, -0.1724,  0.1539],\n",
       "                         [-0.1364, -0.0680,  0.3751, -0.1503, -0.2123],\n",
       "                         [-0.2662, -0.2527,  0.5616,  0.3043, -0.4809],\n",
       "                         [-0.1234, -0.0351, -0.0033,  0.0908, -0.1268],\n",
       "                         [ 0.2929,  0.0086, -0.6046, -0.4618,  0.2399]],\n",
       "               \n",
       "                        [[ 0.2434,  0.0480, -0.1177, -0.0605,  0.1774],\n",
       "                         [-0.0314,  0.1507,  0.3724, -0.0666, -0.0754],\n",
       "                         [-0.0440, -0.0026,  0.4483,  0.4093, -0.2599],\n",
       "                         [ 0.0087,  0.1309, -0.0496,  0.2971, -0.0091],\n",
       "                         [ 0.3782,  0.2109, -0.3730, -0.1412,  0.2410]],\n",
       "               \n",
       "                        [[-0.0057, -0.1017, -0.1919, -0.2214,  0.1021],\n",
       "                         [-0.0918, -0.0093,  0.2730, -0.1076, -0.1697],\n",
       "                         [-0.2561, -0.2990,  0.4533,  0.1639, -0.3027],\n",
       "                         [ 0.0119, -0.0972, -0.0179,  0.1594,  0.0235],\n",
       "                         [ 0.1945,  0.0529, -0.4649, -0.3372,  0.1824]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.4161,  0.4638,  0.3869,  0.0906, -0.0240],\n",
       "                         [ 0.2916,  0.1530,  0.2150, -0.0177,  0.0936],\n",
       "                         [ 0.1786, -0.0357, -0.1406, -0.0516,  0.1478],\n",
       "                         [-0.3244, -0.3390, -0.0556,  0.0576,  0.1455],\n",
       "                         [-0.4465, -0.3874, -0.2348, -0.2123, -0.0263]],\n",
       "               \n",
       "                        [[-0.1528, -0.0892, -0.0449, -0.2562, -0.2098],\n",
       "                         [-0.0932, -0.1474, -0.1519, -0.3188, -0.1072],\n",
       "                         [-0.1893, -0.3671, -0.3064, -0.0384,  0.1128],\n",
       "                         [-0.3493, -0.4418, -0.1963,  0.2342,  0.2738],\n",
       "                         [-0.1642, -0.1213,  0.0374,  0.0602,  0.1991]],\n",
       "               \n",
       "                        [[-0.0376, -0.0308, -0.1324, -0.2050, -0.0850],\n",
       "                         [ 0.0421,  0.0041, -0.1318, -0.1335, -0.0561],\n",
       "                         [-0.0498, -0.2734, -0.0443, -0.0279,  0.2275],\n",
       "                         [-0.1110, -0.3178, -0.0460,  0.2452,  0.3603],\n",
       "                         [-0.0341,  0.1444,  0.2460,  0.2793,  0.4425]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0462, -0.3177,  0.2246,  0.1806, -0.1625],\n",
       "                         [-0.0731, -0.3384,  0.4259,  0.2262, -0.3459],\n",
       "                         [-0.1859, -0.2690,  0.4400,  0.3251, -0.3700],\n",
       "                         [-0.2411, -0.1488,  0.3543,  0.1916, -0.1654],\n",
       "                         [-0.1643, -0.1009,  0.1713,  0.1547, -0.0933]],\n",
       "               \n",
       "                        [[-0.0270, -0.1575,  0.2119,  0.0787, -0.3148],\n",
       "                         [-0.1140, -0.2709,  0.3167,  0.3176, -0.4477],\n",
       "                         [-0.1737, -0.1956,  0.5067,  0.2566, -0.4163],\n",
       "                         [-0.0933, -0.2599,  0.5207,  0.2152, -0.3610],\n",
       "                         [-0.1517,  0.0152,  0.2783,  0.2935, -0.0934]],\n",
       "               \n",
       "                        [[ 0.0803, -0.1982,  0.2262,  0.2997, -0.2215],\n",
       "                         [-0.0472, -0.2229,  0.4212,  0.3097, -0.3657],\n",
       "                         [-0.2658, -0.1634,  0.4564,  0.4031, -0.3849],\n",
       "                         [-0.2644, -0.0242,  0.3017,  0.3514, -0.3299],\n",
       "                         [-0.0122, -0.1302,  0.3335,  0.1609, -0.2121]]]], device='cuda:2')),\n",
       "              ('conv1.bias',\n",
       "               tensor([ 0.3275,  0.5180, -0.3843, -0.0135, -0.3619, -0.1893], device='cuda:2')),\n",
       "              ('conv2.weight',\n",
       "               tensor([[[[-5.7586e-02, -5.2164e-03,  1.3771e-01,  1.2428e-02,  7.8071e-02],\n",
       "                         [-1.2925e-01,  2.2545e-02,  2.7011e-02, -7.4718e-03, -4.7478e-02],\n",
       "                         [-1.4150e-01, -5.8456e-02,  3.1372e-02, -1.0594e-01, -1.7244e-01],\n",
       "                         [-1.0970e-01,  4.5138e-02,  1.1709e-01, -8.8009e-03, -7.8351e-02],\n",
       "                         [-1.2705e-01,  1.2030e-01,  1.7836e-01, -2.7834e-02,  4.6419e-02]],\n",
       "               \n",
       "                        [[ 2.7571e-02,  5.4967e-02,  4.3132e-02,  1.1667e-01,  5.5011e-02],\n",
       "                         [-2.6853e-02,  5.3433e-02,  4.3637e-03,  5.2007e-02,  7.4583e-02],\n",
       "                         [ 6.2640e-03,  6.6561e-02, -6.5543e-02,  5.7918e-02,  1.0867e-01],\n",
       "                         [ 4.4210e-02, -5.0026e-02, -5.5911e-02,  1.0332e-01,  1.9208e-02],\n",
       "                         [-1.2327e-01, -1.5127e-01, -8.6423e-02,  4.8202e-03,  1.2376e-02]],\n",
       "               \n",
       "                        [[-1.5877e-01, -1.7020e-01, -9.3999e-02, -1.8121e-01, -2.4701e-01],\n",
       "                         [-1.6589e-01, -1.9405e-01, -7.6759e-02, -1.0694e-01, -1.8905e-01],\n",
       "                         [-2.3338e-01, -2.1040e-01, -1.8785e-01, -2.0482e-01, -2.4894e-01],\n",
       "                         [-2.4818e-01, -2.0097e-01, -2.6359e-01, -2.5658e-01, -2.8748e-01],\n",
       "                         [-1.7144e-01, -1.9913e-01, -2.1552e-01, -2.2315e-01, -1.9351e-01]],\n",
       "               \n",
       "                        [[-6.9414e-02,  2.9012e-02, -1.0197e-02, -1.0755e-01, -1.1869e-01],\n",
       "                         [ 3.0620e-02,  5.7539e-03, -3.0360e-02,  9.7317e-02,  5.2768e-02],\n",
       "                         [-2.5652e-02,  4.1855e-02,  1.5190e-02,  4.2907e-02,  2.2666e-02],\n",
       "                         [-6.3138e-02,  1.4094e-01,  1.4827e-02,  9.6798e-02, -2.8532e-02],\n",
       "                         [ 1.3036e-02,  1.0490e-01,  1.8735e-02,  1.8132e-02,  3.8292e-02]],\n",
       "               \n",
       "                        [[ 2.9685e-04, -8.8416e-02, -9.9282e-02,  5.6877e-02,  2.4376e-01],\n",
       "                         [-8.8691e-02, -1.3475e-01, -1.9720e-01,  1.8050e-02,  1.7200e-01],\n",
       "                         [-1.2085e-01, -1.7515e-01, -1.9156e-01, -5.1714e-02,  2.7198e-01],\n",
       "                         [-1.1177e-01, -2.2739e-01, -2.2031e-01, -3.4033e-02,  1.4723e-01],\n",
       "                         [ 2.0500e-02, -2.6510e-01, -2.3134e-01, -9.3114e-02,  4.6799e-02]],\n",
       "               \n",
       "                        [[-1.7652e-02,  9.7263e-02,  8.9640e-02, -2.8076e-02, -9.3695e-02],\n",
       "                         [-8.5612e-02,  1.7037e-01,  4.0497e-02,  6.5928e-02, -6.8414e-02],\n",
       "                         [-1.5725e-02,  1.6130e-01,  9.0407e-03, -5.2613e-02, -5.5499e-03],\n",
       "                         [-6.1566e-02,  1.4272e-01, -1.1458e-02,  5.4077e-02,  7.4392e-02],\n",
       "                         [ 6.3202e-02,  2.6802e-01,  7.6422e-02, -1.0422e-01,  1.4327e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 8.5207e-02,  1.2648e-01, -2.3677e-01, -8.3517e-02, -2.4498e-02],\n",
       "                         [ 2.0199e-01, -1.7130e-01, -6.2865e-02,  8.2210e-02,  8.1101e-02],\n",
       "                         [ 5.0685e-03, -1.4210e-01,  6.8497e-02, -9.6519e-02, -7.7289e-02],\n",
       "                         [-1.3690e-01, -1.9006e-02, -9.6796e-02, -3.4780e-02,  5.0824e-02],\n",
       "                         [-5.2474e-02,  2.8627e-02, -2.7935e-02,  1.6289e-01,  6.5989e-02]],\n",
       "               \n",
       "                        [[-2.2494e-01, -1.9504e-01, -2.7997e-01, -1.5971e-01, -3.1735e-03],\n",
       "                         [-1.0850e-01, -2.2865e-01, -2.4181e-01, -6.0972e-02,  1.2829e-01],\n",
       "                         [-1.7385e-01, -7.8892e-02, -4.9829e-02,  2.9078e-02,  1.3048e-01],\n",
       "                         [-1.8734e-01,  6.7895e-02, -6.8087e-02,  2.1268e-02,  4.1123e-02],\n",
       "                         [-1.2143e-01, -8.8890e-02,  1.8287e-02,  4.7429e-02, -6.7574e-03]],\n",
       "               \n",
       "                        [[-5.6387e-02, -8.3123e-02,  9.4163e-02,  5.8203e-02, -2.0417e-01],\n",
       "                         [ 6.8726e-02,  1.7644e-01, -5.6790e-02, -2.4840e-01, -2.0216e-01],\n",
       "                         [ 1.6182e-01, -3.4470e-02, -3.2325e-01, -1.6170e-01,  6.2964e-02],\n",
       "                         [-2.2836e-02, -2.3739e-01, -9.0177e-02, -1.2312e-02,  2.1608e-03],\n",
       "                         [-3.4304e-03, -1.0845e-01,  3.0837e-03, -7.2578e-02,  4.1991e-02]],\n",
       "               \n",
       "                        [[ 1.7875e-01,  2.1886e-01, -1.6087e-01,  8.0610e-02, -2.9312e-02],\n",
       "                         [ 2.1036e-01, -2.2602e-01,  4.2153e-02,  8.5257e-02, -9.5229e-03],\n",
       "                         [-1.0587e-01, -4.7757e-02,  9.8604e-02, -1.2463e-01, -5.3671e-02],\n",
       "                         [-4.0573e-02,  1.0432e-01,  2.8586e-03,  5.5247e-02,  1.6114e-02],\n",
       "                         [-8.7542e-02,  1.8661e-02,  1.2523e-02,  5.1311e-02, -1.1366e-02]],\n",
       "               \n",
       "                        [[-1.7990e-01,  1.2070e-01,  4.8734e-01, -8.2040e-02, -1.4734e-01],\n",
       "                         [-2.2279e-01,  3.6791e-01, -6.5447e-02, -3.1918e-01,  6.5044e-02],\n",
       "                         [ 9.4010e-02,  9.3933e-03, -4.0974e-01, -2.8631e-02,  1.5306e-01],\n",
       "                         [ 1.4875e-02, -2.2127e-01, -1.7875e-01,  1.3107e-01,  4.5641e-02],\n",
       "                         [-1.6557e-01, -1.6947e-01, -3.2160e-02, -1.1621e-01,  2.2885e-01]],\n",
       "               \n",
       "                        [[ 2.2219e-01, -3.3706e-02, -1.5012e-01,  1.9332e-01,  1.5422e-02],\n",
       "                         [ 4.3368e-02, -2.0774e-01,  2.1496e-01, -2.0953e-02, -1.9844e-01],\n",
       "                         [-2.0577e-01,  1.7300e-01,  6.6156e-03, -2.0691e-01,  4.2841e-02],\n",
       "                         [-4.6625e-02,  2.0753e-01, -1.7458e-01,  3.0724e-02,  4.8498e-02],\n",
       "                         [ 9.0442e-02,  7.7418e-02,  3.4512e-02,  1.1411e-01,  1.0262e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 4.5324e-01,  1.4172e-01, -1.2999e-01, -2.6570e-01, -3.0982e-01],\n",
       "                         [ 3.2885e-01,  2.4574e-01, -4.9321e-02, -2.5486e-01, -2.4252e-01],\n",
       "                         [ 2.6016e-01,  2.2607e-01,  6.1716e-02, -1.7015e-01, -3.4014e-01],\n",
       "                         [ 2.5395e-01,  4.2792e-02,  9.5384e-02, -5.6661e-02, -2.7854e-01],\n",
       "                         [ 2.0206e-01,  6.2836e-02,  1.1667e-02, -8.8663e-02, -2.6048e-01]],\n",
       "               \n",
       "                        [[ 1.7594e-01, -8.6418e-02, -2.0216e-01, -3.1774e-01, -2.2215e-01],\n",
       "                         [ 2.4092e-01,  1.5867e-02, -6.3442e-02, -2.5995e-01, -3.3421e-01],\n",
       "                         [ 3.4511e-01,  1.7721e-01,  3.4525e-02, -1.9015e-01, -3.2342e-01],\n",
       "                         [ 3.8916e-01,  1.5696e-01,  3.7686e-02, -2.6520e-01, -3.2830e-01],\n",
       "                         [ 4.2685e-01,  2.6208e-01,  1.6340e-01, -1.2811e-01, -2.2421e-01]],\n",
       "               \n",
       "                        [[ 3.9270e-02,  1.9793e-01, -1.6880e-02, -1.1823e-01, -8.4331e-02],\n",
       "                         [-2.0355e-02,  7.8594e-02,  3.6151e-02, -5.2458e-02, -6.1306e-02],\n",
       "                         [ 5.3365e-02, -5.4259e-02, -5.7617e-02,  6.8220e-03,  6.9350e-02],\n",
       "                         [ 1.1995e-01, -2.3267e-02,  1.5548e-02, -7.2933e-04, -2.6107e-02],\n",
       "                         [ 3.6377e-02, -2.2830e-04, -6.8769e-02, -3.0097e-02,  1.3737e-02]],\n",
       "               \n",
       "                        [[ 6.9370e-03, -9.1447e-02, -9.0961e-02,  7.3391e-02,  1.1553e-01],\n",
       "                         [ 8.3622e-02, -9.3933e-02, -6.4235e-02,  7.3291e-02,  1.6509e-01],\n",
       "                         [-5.0250e-02,  2.4175e-02,  3.6554e-02,  3.5298e-03,  3.5583e-02],\n",
       "                         [-6.5558e-02, -3.3912e-02,  1.0360e-01,  5.5871e-02,  4.5818e-02],\n",
       "                         [-7.2053e-02, -1.1692e-01,  3.2127e-02, -3.1248e-02,  1.0250e-01]],\n",
       "               \n",
       "                        [[-2.8404e-01, -1.3585e-01,  4.2003e-02,  1.1343e-01,  8.2053e-02],\n",
       "                         [-6.2807e-02, -9.5672e-02, -5.4937e-02,  3.2686e-02,  9.7745e-02],\n",
       "                         [-2.4785e-02,  2.7995e-02, -4.2438e-02,  2.2750e-02, -1.0953e-01],\n",
       "                         [-2.4468e-02,  1.5642e-02,  2.3254e-02,  1.4467e-02, -4.1203e-02],\n",
       "                         [-1.1352e-01, -5.2744e-02, -8.7475e-02, -7.5556e-03,  8.0966e-02]],\n",
       "               \n",
       "                        [[-8.3030e-02, -1.3761e-01, -1.5481e-01, -4.0544e-02, -6.8271e-02],\n",
       "                         [ 9.9479e-02,  7.2000e-02, -1.8487e-01, -2.0562e-01, -1.3325e-01],\n",
       "                         [ 9.7485e-02,  1.3123e-01, -5.5140e-02, -3.1030e-01, -2.5604e-01],\n",
       "                         [-5.0237e-02,  1.0638e-01,  5.1843e-02, -2.1029e-01, -2.4832e-01],\n",
       "                         [ 2.6854e-02, -3.1097e-02,  1.0090e-01, -8.9961e-03, -1.6644e-01]]],\n",
       "               \n",
       "               \n",
       "                       ...,\n",
       "               \n",
       "               \n",
       "                       [[[-3.3776e-02,  6.2888e-02,  5.9330e-02, -4.8460e-02,  1.0592e-01],\n",
       "                         [ 2.2517e-03, -2.4016e-02,  6.5361e-02,  1.0229e-01,  8.1555e-02],\n",
       "                         [-3.4863e-02,  1.8709e-02, -4.2909e-02,  9.6196e-02,  1.3286e-01],\n",
       "                         [-2.1048e-02, -4.3694e-02,  1.1347e-01, -2.3579e-03,  1.4577e-02],\n",
       "                         [ 4.8723e-02, -4.4368e-03,  1.0877e-01,  1.1029e-01,  2.8261e-02]],\n",
       "               \n",
       "                        [[-2.9579e-02,  4.8771e-02, -1.3811e-02,  6.7324e-03, -3.5695e-02],\n",
       "                         [-7.2909e-02, -4.6744e-04, -4.8395e-02, -4.5189e-02, -4.4519e-02],\n",
       "                         [ 6.5466e-02, -5.8217e-02,  2.5734e-02,  3.5922e-02,  8.9584e-02],\n",
       "                         [ 3.6227e-02, -3.6374e-02,  5.9235e-02, -1.0056e-02,  3.7241e-02],\n",
       "                         [-3.5607e-02, -3.6832e-02,  8.9269e-02, -6.6546e-03,  1.3705e-01]],\n",
       "               \n",
       "                        [[-1.1084e-02, -8.8720e-02, -1.2734e-01, -1.3651e-01, -1.0965e-01],\n",
       "                         [-1.0580e-01, -7.7549e-02, -9.1215e-02, -4.7561e-02, -1.1756e-01],\n",
       "                         [-3.7951e-02, -8.1079e-02, -5.8799e-02, -1.9960e-02, -1.4140e-01],\n",
       "                         [-1.0091e-01,  9.6909e-05, -5.4141e-02, -5.8126e-02, -8.3445e-02],\n",
       "                         [-1.5322e-02, -4.4215e-02, -7.0257e-02, -5.1304e-02, -3.5490e-03]],\n",
       "               \n",
       "                        [[-1.8074e-02, -1.0684e-01,  8.4888e-04, -1.5221e-01, -6.9281e-02],\n",
       "                         [-4.3720e-02, -1.1697e-01, -2.8295e-02,  9.7194e-03, -5.1293e-02],\n",
       "                         [ 5.8880e-02, -4.2294e-02, -4.3054e-02,  5.2699e-02,  7.7010e-02],\n",
       "                         [ 8.3618e-02,  6.9452e-02, -4.5588e-02,  4.7440e-02,  3.6032e-02],\n",
       "                         [-4.7178e-02,  1.1683e-01,  7.5831e-02, -4.2394e-02, -1.8945e-02]],\n",
       "               \n",
       "                        [[-7.2453e-02, -2.5869e-03, -1.0876e-01, -6.9804e-02, -4.5326e-02],\n",
       "                         [ 3.6788e-02, -8.6417e-02,  5.7428e-02, -5.8424e-02, -9.3316e-02],\n",
       "                         [ 7.5093e-02, -4.7498e-02,  7.4688e-02,  4.8116e-02, -3.3693e-02],\n",
       "                         [ 1.9037e-02,  3.4316e-02, -1.5353e-02,  3.1404e-03,  7.2664e-02],\n",
       "                         [-5.9230e-02,  1.1893e-03,  1.4350e-03,  7.8745e-02,  1.0757e-01]],\n",
       "               \n",
       "                        [[-9.5639e-02, -1.0605e-01, -9.5268e-02, -3.9527e-02, -1.0111e-01],\n",
       "                         [-6.2909e-02, -9.9454e-02, -7.2710e-02, -1.1742e-01,  2.5461e-02],\n",
       "                         [-1.6462e-02, -2.9695e-02, -8.7325e-02,  5.2300e-02, -3.3498e-02],\n",
       "                         [-6.6865e-03,  1.1630e-01,  7.6257e-02, -6.7460e-02,  7.8170e-02],\n",
       "                         [-4.7005e-02,  8.5784e-02,  1.3651e-01,  3.5644e-02,  1.4477e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.4977e-01, -1.3227e-01, -8.9533e-02, -5.9722e-03, -4.1642e-02],\n",
       "                         [-2.5565e-01, -1.6364e-01, -5.5677e-02,  5.2913e-02, -1.4205e-02],\n",
       "                         [-2.5325e-01, -1.5185e-01,  3.3657e-02,  2.9797e-02,  6.5358e-02],\n",
       "                         [-6.5036e-03, -1.2222e-01, -4.8757e-02,  2.7002e-02, -4.3572e-02],\n",
       "                         [ 4.7039e-02,  1.1773e-01,  7.1120e-03,  1.6451e-01,  7.9840e-02]],\n",
       "               \n",
       "                        [[ 2.1680e-01,  2.0266e-01,  3.5874e-02,  1.5227e-01, -2.0727e-01],\n",
       "                         [ 1.0787e-01,  6.6879e-02,  1.4101e-01,  4.8611e-02, -1.8542e-01],\n",
       "                         [ 1.3752e-02,  7.6270e-02,  6.7109e-04, -9.0854e-02, -1.3155e-01],\n",
       "                         [ 3.1205e-02, -4.3697e-03, -5.1262e-02, -1.1021e-01, -1.4904e-01],\n",
       "                         [ 8.7112e-02,  2.9850e-02, -6.4971e-02, -7.8095e-02, -1.4268e-01]],\n",
       "               \n",
       "                        [[-1.8143e-01, -2.9881e-02,  1.1296e-02, -4.6418e-02,  5.7484e-02],\n",
       "                         [-3.5703e-02, -1.5139e-01, -2.8321e-02, -6.2728e-02,  8.0499e-02],\n",
       "                         [ 1.2335e-01,  6.6426e-03, -8.0287e-03,  1.0342e-02,  8.7282e-03],\n",
       "                         [ 3.8299e-03,  1.5477e-01,  9.4254e-02, -3.7976e-02,  1.7458e-02],\n",
       "                         [ 1.1739e-01,  5.2034e-02,  1.3094e-01, -2.4704e-02, -1.6255e-02]],\n",
       "               \n",
       "                        [[ 2.6324e-01,  2.1924e-02,  3.1448e-02,  4.3837e-02, -2.3645e-02],\n",
       "                         [ 2.5143e-01,  1.2016e-01,  7.0890e-02,  2.9235e-02, -8.3176e-02],\n",
       "                         [ 1.4216e-01,  9.7938e-02,  2.3688e-02,  1.7474e-02, -1.2630e-01],\n",
       "                         [ 1.6821e-01,  1.5949e-02, -1.8880e-02, -1.2356e-01, -1.8405e-01],\n",
       "                         [-1.3896e-01, -9.6003e-02, -2.2452e-01, -2.1298e-01, -2.8169e-01]],\n",
       "               \n",
       "                        [[ 2.3080e-01, -2.9267e-02,  1.0498e-02,  7.1980e-02,  2.0204e-01],\n",
       "                         [ 9.1331e-02,  9.8812e-02,  2.3662e-02, -1.7242e-02,  7.9148e-02],\n",
       "                         [-3.9100e-02,  1.9039e-01,  1.5752e-01,  3.2106e-02,  8.4344e-02],\n",
       "                         [ 8.4948e-02,  3.7478e-02,  1.4042e-01,  2.3650e-01,  4.0316e-02],\n",
       "                         [-3.3485e-02, -1.1492e-01, -9.4552e-02,  8.5218e-02, -9.4891e-02]],\n",
       "               \n",
       "                        [[ 2.5655e-01,  1.5533e-01, -2.6957e-02,  7.6771e-02,  1.7579e-02],\n",
       "                         [ 1.8746e-01,  5.2715e-02,  1.7575e-01,  2.8040e-03,  9.5341e-03],\n",
       "                         [ 7.8925e-02,  1.0652e-01,  2.4393e-02,  3.7045e-03,  5.8375e-02],\n",
       "                         [ 4.9789e-02, -8.6571e-02, -5.2561e-02, -8.8946e-02, -4.1123e-02],\n",
       "                         [-1.4360e-01, -2.2074e-01, -2.5254e-01, -2.0910e-01, -1.2641e-01]]],\n",
       "               \n",
       "               \n",
       "                       [[[-1.5028e-01, -6.4533e-03, -1.7771e-02,  4.4848e-02,  9.5914e-02],\n",
       "                         [-1.5409e-01, -7.8128e-02, -5.3922e-02,  1.3623e-01, -1.5218e-01],\n",
       "                         [-8.9365e-02,  1.1202e-01,  1.1388e-01,  1.0558e-02, -2.2524e-01],\n",
       "                         [ 1.5004e-01,  1.3143e-01,  6.4656e-02, -1.3464e-01,  3.8118e-02],\n",
       "                         [ 1.4406e-01, -2.5350e-02, -9.9373e-02, -8.4488e-04,  1.2082e-01]],\n",
       "               \n",
       "                        [[-2.7239e-03,  6.6734e-02,  1.1245e-01,  2.4871e-02, -2.8688e-02],\n",
       "                         [-1.7610e-02,  1.3005e-01,  1.7047e-02,  6.0413e-03,  2.0115e-02],\n",
       "                         [ 1.3734e-02,  1.2309e-01,  1.0490e-01, -9.6527e-02, -1.1038e-01],\n",
       "                         [-1.4733e-02,  9.7408e-03, -3.4828e-02, -8.4745e-02, -2.6145e-02],\n",
       "                         [-1.7942e-01, -1.5255e-01, -1.0403e-01, -6.7193e-03,  2.9397e-02]],\n",
       "               \n",
       "                        [[-9.1955e-02, -7.2018e-02,  1.2638e-01,  3.4529e-02, -6.5922e-02],\n",
       "                         [-1.0754e-01,  5.9358e-02, -8.3546e-02, -2.0411e-02,  6.1604e-04],\n",
       "                         [-2.6818e-02, -1.1283e-01, -1.8162e-01, -2.6836e-03,  1.5046e-01],\n",
       "                         [-1.6788e-01, -7.3933e-02,  2.3727e-01,  7.9345e-02, -5.0061e-02],\n",
       "                         [ 8.9104e-02,  1.2975e-01,  4.7823e-02, -1.0812e-01, -2.1772e-01]],\n",
       "               \n",
       "                        [[ 2.3917e-02,  2.9215e-02, -1.2969e-01,  5.5786e-02,  1.6666e-01],\n",
       "                         [-8.0802e-02, -1.2409e-01,  5.3134e-02,  1.8172e-01,  5.2461e-02],\n",
       "                         [-1.0956e-01,  2.5134e-02,  2.6336e-01, -1.0282e-02, -3.3522e-02],\n",
       "                         [ 8.5174e-02,  1.4692e-01, -9.4227e-02, -8.4670e-02,  9.1260e-02],\n",
       "                         [-5.5994e-02, -1.6840e-01, -1.4307e-01,  3.4789e-02, -8.1848e-02]],\n",
       "               \n",
       "                        [[-1.2439e-02, -5.6620e-02,  3.0043e-02, -1.8405e-01, -4.9629e-03],\n",
       "                         [ 1.7210e-02, -7.5543e-02, -2.5557e-01, -1.9814e-01,  3.9931e-01],\n",
       "                         [-5.7865e-02, -2.3637e-01, -2.3823e-01,  3.2908e-01,  5.2018e-01],\n",
       "                         [-2.9416e-01, -2.5994e-02,  3.5754e-01,  3.8566e-01, -6.0330e-02],\n",
       "                         [ 1.8432e-01,  2.3405e-01,  1.5324e-01, -2.3591e-01, -2.3072e-01]],\n",
       "               \n",
       "                        [[-9.3306e-02, -5.4631e-02, -1.1325e-01,  1.8848e-02, -1.3108e-01],\n",
       "                         [ 1.8823e-02, -3.7554e-02,  7.7126e-02, -2.0716e-01, -2.0911e-01],\n",
       "                         [ 1.2423e-01,  1.7408e-02, -1.0052e-01, -2.2772e-01, -1.8481e-04],\n",
       "                         [-6.7027e-02,  9.7294e-03, -1.7324e-01,  6.5503e-02,  1.3690e-01],\n",
       "                         [-1.9674e-01, -2.5735e-01, -1.2364e-01, -3.5882e-02, -7.5740e-03]]]],\n",
       "                      device='cuda:2')),\n",
       "              ('conv2.bias',\n",
       "               tensor([ 0.9995,  0.4522,  0.0295, -0.0957,  0.0300,  0.2850,  0.1562, -0.3812,\n",
       "                       -0.3958, -0.0656, -0.0509, -0.5655,  0.2151,  0.0755, -0.0807, -0.3398],\n",
       "                      device='cuda:2')),\n",
       "              ('fc1.weight',\n",
       "               tensor([[ 0.0303, -0.0650, -0.0799,  ..., -0.0268,  0.0181, -0.0048],\n",
       "                       [-0.0167, -0.0530,  0.0030,  ..., -0.0539, -0.0223,  0.0378],\n",
       "                       [ 0.0202, -0.0390,  0.0294,  ..., -0.0326, -0.0056,  0.0359],\n",
       "                       ...,\n",
       "                       [-0.0079, -0.0409, -0.0358,  ...,  0.0076, -0.0537, -0.0138],\n",
       "                       [ 0.0615,  0.0370, -0.0122,  ...,  0.0127, -0.0327, -0.0188],\n",
       "                       [-0.0688, -0.0126,  0.0594,  ...,  0.0094, -0.0341,  0.0214]],\n",
       "                      device='cuda:2')),\n",
       "              ('fc1.bias',\n",
       "               tensor([ 0.1414,  0.0033, -0.0471, -0.0974,  0.0489,  0.0468,  0.0162,  0.0083,\n",
       "                       -0.0455, -0.0203,  0.2121,  0.0669,  0.0149, -0.0351,  0.1204, -0.0820,\n",
       "                       -0.0559,  0.0845, -0.0103,  0.0355,  0.1697,  0.0364, -0.0511,  0.0056,\n",
       "                        0.1307,  0.0408, -0.0666,  0.1161,  0.0740,  0.0683,  0.0057,  0.0247,\n",
       "                       -0.0524,  0.0268, -0.0516, -0.0799,  0.0113, -0.0059,  0.0067, -0.1108,\n",
       "                        0.0717,  0.0483,  0.0161, -0.0234, -0.0224, -0.0493,  0.0721,  0.2062,\n",
       "                        0.0129, -0.0639, -0.0082,  0.0262, -0.0384,  0.0075, -0.2057,  0.0222,\n",
       "                       -0.0235,  0.1328, -0.1251, -0.0214,  0.0089,  0.0691, -0.0557,  0.0387,\n",
       "                       -0.0498, -0.0344, -0.0078,  0.1752,  0.0394,  0.0921, -0.0236, -0.0078,\n",
       "                        0.0742, -0.0062,  0.1487,  0.0307,  0.0400, -0.1266,  0.1591,  0.0527,\n",
       "                        0.0271,  0.0579, -0.0515, -0.0062, -0.0310,  0.1375,  0.0559,  0.1300,\n",
       "                        0.0288,  0.0444,  0.0209, -0.0878, -0.0310, -0.0915,  0.1186, -0.0260,\n",
       "                       -0.0186,  0.1190,  0.0118, -0.0106,  0.1160,  0.0075,  0.0526,  0.0706,\n",
       "                        0.0562,  0.0939, -0.0373,  0.1277,  0.0155, -0.1242,  0.0060, -0.0033,\n",
       "                        0.0260, -0.0673, -0.0233,  0.1830,  0.0759, -0.0134, -0.0195, -0.0143],\n",
       "                      device='cuda:2')),\n",
       "              ('fc2.weight',\n",
       "               tensor([[-0.0661,  0.0077, -0.1099,  ...,  0.0630,  0.0146, -0.0051],\n",
       "                       [ 0.1361,  0.0378,  0.0509,  ..., -0.0016, -0.0675, -0.0767],\n",
       "                       [ 0.0889, -0.0527, -0.1030,  ...,  0.0580, -0.0118, -0.0115],\n",
       "                       ...,\n",
       "                       [ 0.0918,  0.0909, -0.0756,  ..., -0.0269,  0.0161, -0.0752],\n",
       "                       [ 0.1205,  0.0797,  0.0883,  ...,  0.0203, -0.1125,  0.0390],\n",
       "                       [ 0.1398, -0.0659,  0.0884,  ...,  0.0222, -0.0874,  0.0362]],\n",
       "                      device='cuda:2')),\n",
       "              ('fc2.bias',\n",
       "               tensor([ 7.3059e-02,  2.3885e-01,  1.5243e-02,  9.8529e-02, -4.1907e-03,\n",
       "                        4.5932e-02, -8.6891e-02,  1.9579e-01,  2.2556e-01,  1.9320e-01,\n",
       "                       -6.3325e-05, -4.0407e-02, -6.0775e-03,  2.7660e-02, -1.3553e-01,\n",
       "                       -2.4431e-02, -3.7060e-02, -1.8935e-01,  4.0951e-02, -7.8290e-03,\n",
       "                        9.3773e-02, -9.7966e-03,  2.7022e-01,  7.5426e-02,  3.2910e-02,\n",
       "                        1.0527e-02,  2.7371e-01,  2.1137e-01,  1.0957e-01, -1.2576e-01,\n",
       "                        1.3355e-01, -2.3478e-02, -9.6092e-02, -5.3007e-02,  3.2520e-01,\n",
       "                       -1.5646e-01,  4.4937e-02,  8.8718e-02,  1.1643e-01, -1.5257e-01,\n",
       "                        3.6126e-02, -3.7445e-02, -5.1889e-02,  1.4679e-01,  1.0189e-01,\n",
       "                       -9.0278e-02,  1.1929e-01,  4.6515e-02,  5.3004e-02, -4.0802e-02,\n",
       "                       -2.5652e-01,  1.1328e-01, -3.1838e-02, -7.1097e-02, -6.5037e-02,\n",
       "                        6.1825e-03, -3.8481e-02,  3.4245e-02,  1.4070e-01, -3.9686e-02,\n",
       "                        2.7085e-02,  6.0203e-03,  1.8920e-01,  3.8330e-02,  6.4282e-03,\n",
       "                       -1.7384e-01, -4.3417e-03, -9.9598e-02,  1.8538e-01,  2.4672e-01,\n",
       "                       -1.7774e-01,  1.5859e-01,  1.2557e-01,  4.5281e-02,  1.8413e-01,\n",
       "                       -4.2689e-02,  1.1909e-01, -3.7533e-02,  1.2168e-01,  6.1933e-02,\n",
       "                       -6.4670e-02, -3.1469e-02, -1.0791e-01,  2.6383e-01], device='cuda:2')),\n",
       "              ('fc3.weight',\n",
       "               tensor([[-6.7705e-02,  3.3506e-03,  1.1555e-01, -4.4864e-02, -1.0426e-01,\n",
       "                        -2.5731e-01, -7.3967e-02,  1.4367e-01,  1.8755e-01,  2.1188e-01,\n",
       "                        -1.3454e-01, -9.1103e-03, -1.5092e-01,  7.7352e-02,  1.0088e-01,\n",
       "                         5.3208e-02,  1.1111e-01, -8.2465e-02, -1.2740e-01, -1.1039e-01,\n",
       "                        -1.7490e-01,  3.4005e-01, -6.8162e-02, -1.4020e-02,  6.3191e-02,\n",
       "                         2.6997e-01, -2.9989e-02,  1.0042e-01, -1.4243e-01,  1.2885e-01,\n",
       "                        -3.2552e-02,  1.8210e-01, -8.3645e-02,  1.4771e-01, -1.4575e-01,\n",
       "                         1.2848e-01,  1.2169e-01,  1.4904e-01,  2.5129e-01, -1.8232e-01,\n",
       "                         2.8034e-02,  6.5624e-02, -1.2987e-01,  9.1338e-02, -1.3613e-01,\n",
       "                         2.0105e-01, -1.8167e-01, -1.6408e-01, -1.9573e-01,  7.6141e-02,\n",
       "                         1.3293e-01, -2.4478e-01, -4.7460e-02,  4.2355e-02, -2.3498e-01,\n",
       "                         3.0667e-02,  1.1264e-01, -5.3309e-02,  3.4496e-02, -2.4924e-02,\n",
       "                        -4.7917e-02, -4.9867e-02,  3.2576e-01, -1.0114e-01, -9.5054e-02,\n",
       "                         4.1689e-02,  1.0493e-01,  7.6594e-02, -1.5603e-01, -1.5068e-02,\n",
       "                         3.7335e-02, -6.4784e-02,  1.3994e-02,  1.1602e-01, -2.6896e-02,\n",
       "                         1.6697e-01,  1.3177e-01,  4.3220e-02, -1.3872e-01, -5.7452e-02,\n",
       "                         4.6117e-02,  4.0865e-02,  2.0191e-02, -2.1595e-01],\n",
       "                       [ 4.4173e-02, -8.3650e-02,  8.1049e-02, -2.3045e-02,  6.3120e-02,\n",
       "                         1.6354e-01,  1.2827e-01, -2.9760e-01, -2.5787e-01, -1.7939e-01,\n",
       "                         1.9112e-02,  1.4481e-01,  4.7633e-02,  6.5317e-03,  2.4989e-01,\n",
       "                         2.4478e-01, -6.2122e-02,  1.6420e-01, -4.4002e-02,  7.8306e-04,\n",
       "                        -6.3537e-02, -6.5364e-02, -1.2126e-01, -1.0867e-01,  9.7017e-02,\n",
       "                         4.9742e-02, -2.8529e-01,  2.7801e-02, -4.7082e-02,  5.1528e-02,\n",
       "                        -7.0884e-02,  2.5738e-01,  1.0884e-01, -8.0829e-02, -1.6877e-01,\n",
       "                         3.1910e-01,  4.4392e-02, -3.6243e-02, -1.9872e-01,  1.4028e-01,\n",
       "                        -2.4553e-01,  3.0846e-01, -8.7196e-02,  5.1116e-02, -2.1520e-02,\n",
       "                        -2.5829e-02,  4.3491e-03, -2.9908e-01,  1.9611e-02,  1.1407e-01,\n",
       "                         1.1872e-01, -2.6717e-02,  1.8032e-01, -3.0437e-02,  6.0300e-02,\n",
       "                         1.4080e-02, -2.2716e-02, -1.9156e-02, -2.4938e-01,  5.5091e-03,\n",
       "                        -6.4334e-02, -2.5122e-02, -5.0198e-02, -6.2630e-02,  5.6255e-02,\n",
       "                         1.7757e-01, -3.2904e-02,  5.2053e-02,  5.3019e-02, -2.8580e-01,\n",
       "                         3.8617e-02, -1.1250e-01,  5.7019e-02, -5.5543e-02, -1.9851e-02,\n",
       "                        -1.7583e-01,  7.6291e-02, -1.0584e-01, -2.6177e-01,  4.8449e-02,\n",
       "                         2.6016e-01,  8.4327e-02,  1.1510e-01, -2.9702e-01],\n",
       "                       [-2.4969e-01,  8.3128e-02, -2.0280e-01, -1.8520e-02,  5.2227e-02,\n",
       "                         9.2036e-02,  1.1083e-01,  1.6605e-01,  1.5112e-01,  2.9128e-01,\n",
       "                        -4.6220e-02, -8.6553e-02, -3.5201e-02, -1.3620e-01, -6.2333e-02,\n",
       "                         9.0351e-03, -4.8731e-02, -2.6682e-02,  1.8044e-01,  1.0978e-01,\n",
       "                        -1.2585e-01,  6.6704e-02, -2.3139e-02,  9.6913e-02, -3.1384e-02,\n",
       "                        -6.0465e-02,  2.7328e-01,  1.1148e-01,  1.1293e-01,  4.5358e-02,\n",
       "                         1.6631e-01, -1.8946e-02, -9.7156e-02,  4.7761e-02,  1.4083e-01,\n",
       "                         8.3180e-02, -1.2726e-01, -1.7427e-01, -1.3062e-01, -2.1046e-02,\n",
       "                         1.9141e-01, -2.7128e-01,  2.2166e-01, -1.4067e-01,  9.2813e-02,\n",
       "                         1.2765e-02, -1.4353e-01,  4.5699e-02, -1.0922e-01,  4.4767e-02,\n",
       "                        -2.0697e-02,  4.6075e-02,  5.4448e-02,  1.0448e-01,  1.7344e-01,\n",
       "                         4.5071e-02,  3.7028e-02,  3.8978e-02,  1.2387e-01,  1.3689e-01,\n",
       "                         1.5244e-01,  8.3386e-02,  2.1669e-01, -8.7456e-03, -3.0554e-02,\n",
       "                        -9.4139e-02,  2.5478e-01,  5.7921e-02, -4.1950e-02, -2.5579e-02,\n",
       "                        -1.8764e-01, -5.6644e-02, -7.6621e-02,  3.5727e-01, -1.9974e-01,\n",
       "                        -3.0091e-02,  9.2013e-02,  3.6176e-02, -1.6216e-01,  9.3922e-02,\n",
       "                        -8.4565e-02,  8.7807e-02,  1.6886e-01,  1.2639e-01],\n",
       "                       [-1.2519e-01, -9.6159e-02, -8.1092e-02, -1.7886e-01,  9.6566e-03,\n",
       "                         2.6185e-02, -8.5201e-03,  1.8591e-01, -7.4933e-02, -8.7563e-02,\n",
       "                         1.1032e-01, -4.4498e-02, -1.3378e-01, -6.5998e-02, -1.2752e-01,\n",
       "                         7.3627e-02, -7.2434e-02,  9.9214e-02, -4.1730e-02,  2.2700e-02,\n",
       "                        -1.1483e-01,  1.8054e-02,  1.8737e-01, -4.7000e-03, -1.7117e-01,\n",
       "                         1.7337e-02,  2.0724e-02, -1.3287e-01,  5.5776e-02,  6.5042e-03,\n",
       "                        -5.2717e-02, -2.8525e-01, -3.2052e-02, -1.2242e-01,  1.7293e-01,\n",
       "                        -1.1948e-01,  4.6044e-02, -1.3173e-01, -1.1137e-01,  9.8712e-02,\n",
       "                         1.4977e-01, -4.0650e-02,  4.1939e-04,  3.2027e-02,  1.8520e-01,\n",
       "                        -1.4006e-01,  2.8021e-02,  1.5438e-02,  1.3167e-01, -1.8942e-01,\n",
       "                        -8.3388e-02, -3.6859e-02, -4.0304e-02, -2.6131e-03,  1.0059e-01,\n",
       "                        -7.7613e-02,  2.1807e-02, -9.4432e-03,  1.0304e-01,  7.0103e-02,\n",
       "                         5.5610e-02,  3.7139e-03,  2.3077e-02, -5.8309e-02,  8.4296e-02,\n",
       "                        -1.5086e-01, -4.9988e-02, -9.4660e-02, -2.2759e-01,  1.2665e-01,\n",
       "                         1.0172e-01, -6.6503e-02,  5.2020e-02, -1.4837e-01,  1.3322e-01,\n",
       "                         4.5829e-02,  1.2515e-01, -5.1120e-02,  2.1161e-02,  2.7938e-02,\n",
       "                        -7.9488e-03,  5.6094e-02, -9.4435e-02,  2.8020e-01],\n",
       "                       [ 1.9206e-02,  3.8463e-01,  1.3414e-01,  2.1715e-01,  1.0439e-01,\n",
       "                        -1.0853e-01, -1.0309e-01,  1.5724e-01,  2.8940e-01,  7.5561e-02,\n",
       "                        -4.5418e-02,  6.5276e-02, -3.0798e-02, -1.2280e-01,  2.6292e-02,\n",
       "                         1.1353e-01, -5.0620e-03, -2.1350e-01,  1.8789e-01,  2.0579e-01,\n",
       "                        -3.5976e-02, -5.7993e-02,  2.3534e-01,  4.0618e-02, -3.8471e-02,\n",
       "                        -1.1834e-02,  1.0810e-01,  3.0849e-01,  8.1780e-02, -2.8070e-01,\n",
       "                        -2.1045e-01, -1.0036e-01,  4.5385e-02, -8.4515e-02,  1.7663e-01,\n",
       "                        -1.0780e-01, -4.3543e-02,  3.1573e-03,  3.9784e-02, -1.5505e-01,\n",
       "                        -2.4982e-01, -2.6284e-01, -6.6883e-02, -1.1051e-01,  5.1254e-03,\n",
       "                        -1.3182e-01,  5.9635e-02, -5.0667e-02, -5.9796e-03, -1.4199e-01,\n",
       "                        -8.8043e-02,  1.8801e-01, -9.1961e-02,  1.0884e-01, -1.3464e-01,\n",
       "                        -1.0825e-01, -3.0941e-02, -3.4658e-02, -5.2136e-02, -8.2060e-02,\n",
       "                        -1.7685e-01, -9.3610e-02, -3.0053e-02, -4.3146e-02,  8.7661e-02,\n",
       "                        -1.1278e-02,  7.3240e-02, -7.3383e-02,  1.5378e-01,  1.4781e-01,\n",
       "                        -9.1359e-02,  9.8622e-02,  6.0366e-02, -1.9258e-01, -3.2059e-01,\n",
       "                        -9.2975e-02, -4.0985e-02, -1.0429e-01,  1.6287e-01,  1.0781e-01,\n",
       "                        -2.9842e-01,  7.6186e-02, -1.7871e-01,  1.4848e-01],\n",
       "                       [-2.3452e-02, -4.9130e-02, -9.4749e-02, -7.9748e-02, -9.4545e-03,\n",
       "                         1.4021e-01,  9.7591e-02,  1.5776e-01, -8.2540e-02, -1.3492e-01,\n",
       "                        -5.0085e-02,  4.4823e-02, -4.3715e-02, -1.3527e-01,  4.8531e-02,\n",
       "                        -2.4433e-01,  7.1933e-02,  1.2310e-01, -9.3911e-02,  1.1029e-01,\n",
       "                         9.6506e-03,  5.3051e-02,  1.2885e-01,  4.7992e-02, -1.4229e-01,\n",
       "                        -1.0623e-01, -6.9611e-02, -1.3921e-01,  1.2651e-01,  5.8736e-02,\n",
       "                         4.0952e-02, -1.2361e-01,  7.3077e-02, -4.5613e-02,  6.9356e-02,\n",
       "                        -1.1017e-01, -5.5233e-02,  1.3440e-01, -2.1052e-01,  2.6676e-02,\n",
       "                         2.7666e-01, -1.6788e-01,  1.0230e-02, -8.4651e-02,  2.6181e-01,\n",
       "                        -1.7010e-01,  1.9406e-01,  1.8927e-01, -1.3820e-01, -9.4204e-02,\n",
       "                        -7.0373e-02,  1.4610e-01, -1.7180e-01, -1.5388e-01, -1.1032e-01,\n",
       "                         3.3911e-01, -3.9413e-02, -3.0658e-03, -4.3330e-02,  1.1461e-01,\n",
       "                         3.1436e-02,  4.8460e-02, -1.6047e-01, -3.0559e-02,  2.2921e-03,\n",
       "                        -8.5201e-02,  5.3583e-02, -5.6119e-02,  4.7051e-02,  1.9682e-01,\n",
       "                         5.5483e-02, -1.9667e-01, -6.0315e-02,  5.8103e-02,  8.2382e-02,\n",
       "                        -7.6211e-03, -1.2810e-01,  1.2407e-01,  8.4920e-02,  1.4816e-01,\n",
       "                         3.6580e-02,  2.4476e-02, -6.4415e-02,  1.2889e-01],\n",
       "                       [ 1.9221e-01,  7.0081e-02,  2.8934e-02, -1.6609e-01,  4.3147e-02,\n",
       "                         1.4209e-01,  4.8625e-02, -1.3061e-01, -8.0432e-02,  6.7002e-02,\n",
       "                         1.6066e-01,  5.0988e-02, -3.2272e-01, -2.4191e-01,  5.4738e-02,\n",
       "                        -2.2286e-01,  6.6487e-02, -2.1424e-01, -1.9378e-02, -1.7467e-01,\n",
       "                         3.4720e-01,  9.4016e-02,  9.3871e-02, -8.8124e-02, -4.5536e-02,\n",
       "                        -3.2892e-01,  1.8173e-01,  1.9203e-01, -9.7246e-02,  2.0898e-01,\n",
       "                         1.2884e-01, -8.1095e-02,  9.6969e-02,  2.7470e-01,  3.4918e-01,\n",
       "                        -8.7221e-02,  6.9292e-03, -2.5832e-02,  3.2315e-03, -8.4922e-03,\n",
       "                        -4.5431e-02, -3.8232e-02,  2.7878e-02, -1.9747e-02, -1.2064e-01,\n",
       "                         1.8650e-04, -9.8987e-02, -6.8691e-02,  1.7468e-01, -2.7263e-01,\n",
       "                        -2.0015e-01, -1.7239e-01,  1.5771e-02,  7.3828e-02, -1.3123e-01,\n",
       "                         1.8871e-02, -8.2079e-02,  4.7575e-02,  2.3724e-01,  8.7778e-02,\n",
       "                         1.8878e-01, -7.8215e-02, -1.9396e-01,  1.2389e-01,  1.0331e-01,\n",
       "                        -1.5107e-01, -1.0076e-01, -6.3452e-02,  3.6116e-01,  1.7638e-01,\n",
       "                         6.4353e-03, -7.5237e-02, -1.9150e-01, -1.9618e-01, -5.1068e-02,\n",
       "                        -1.3494e-01,  2.6890e-01, -1.4024e-01, -8.2939e-02, -1.6995e-01,\n",
       "                         1.7788e-02, -1.7413e-02,  1.3638e-01,  1.6775e-01],\n",
       "                       [ 6.1049e-03, -5.1122e-02,  2.0254e-01,  3.2982e-03, -1.0879e-01,\n",
       "                        -5.0432e-02, -5.2195e-02,  6.1449e-02, -1.7611e-01, -2.3619e-01,\n",
       "                         9.8508e-02,  9.6895e-03,  3.2383e-01, -1.9006e-01, -5.3857e-02,\n",
       "                        -6.0956e-02, -2.4663e-03, -4.0698e-02,  9.6381e-02,  9.4625e-02,\n",
       "                         8.0144e-02, -6.4894e-02,  1.4070e-02, -5.0425e-02,  2.3440e-01,\n",
       "                        -7.3877e-02, -2.5191e-02, -5.9861e-02,  1.7819e-01, -1.3520e-01,\n",
       "                        -1.0001e-01, -7.7290e-02, -2.1864e-02,  8.6394e-02, -2.2594e-01,\n",
       "                        -1.9598e-01,  6.6719e-02, -2.8168e-01,  1.2638e-02,  6.1152e-02,\n",
       "                         2.0474e-01, -1.9913e-01,  1.2462e-01, -7.6999e-02, -3.2148e-02,\n",
       "                         1.9324e-01,  1.9028e-01, -7.4976e-02, -3.3726e-01,  1.4814e-01,\n",
       "                         2.5996e-01,  4.4135e-02,  8.7327e-02,  1.3669e-02,  1.3436e-02,\n",
       "                        -1.3253e-01,  2.7375e-02, -1.7486e-01,  4.7175e-02,  2.1495e-02,\n",
       "                        -1.7922e-01,  8.6517e-02, -2.5595e-01, -3.9674e-02, -7.5788e-02,\n",
       "                         7.4464e-02,  2.6415e-01,  9.0430e-02,  7.2467e-02,  6.6189e-02,\n",
       "                         1.2755e-01,  2.2166e-01, -2.0180e-01, -1.2443e-01, -5.4189e-02,\n",
       "                         1.5342e-01, -1.3237e-01,  1.3216e-01,  1.4950e-01,  1.7266e-01,\n",
       "                        -1.6722e-01,  2.4821e-02,  6.5404e-02,  2.0726e-01],\n",
       "                       [ 4.4631e-02, -3.0807e-02, -1.8299e-01, -7.3713e-02, -1.0238e-02,\n",
       "                        -1.9181e-01, -3.5207e-02, -9.0455e-02,  1.2878e-01,  1.2470e-01,\n",
       "                         9.3569e-02,  1.4316e-02,  1.4235e-01,  3.2181e-01, -1.8668e-01,\n",
       "                         1.4375e-01,  1.9905e-02, -1.4203e-01, -7.9729e-03, -2.4330e-01,\n",
       "                        -2.5402e-01, -1.9269e-01, -3.0227e-01, -1.8951e-02, -3.1055e-02,\n",
       "                         1.7248e-01,  8.6062e-02, -2.0152e-01,  7.1177e-02, -1.6531e-01,\n",
       "                         2.5632e-01,  1.9661e-01, -8.6502e-02, -1.9063e-01, -8.7460e-02,\n",
       "                         5.7610e-02, -8.8356e-02,  8.1320e-02,  4.3926e-02, -2.8681e-02,\n",
       "                        -2.5260e-01,  2.1320e-01, -2.2430e-02,  2.6136e-01,  5.3455e-02,\n",
       "                        -2.0259e-02,  5.8641e-02, -7.2184e-02,  3.2697e-02,  2.6178e-01,\n",
       "                        -2.4119e-01, -2.3954e-01, -2.6409e-01,  7.5750e-03, -1.5439e-02,\n",
       "                        -6.4607e-02,  5.6323e-02,  9.2000e-02, -2.0161e-01, -1.2299e-02,\n",
       "                         2.1061e-02,  6.9730e-02,  2.0664e-01,  9.4379e-02, -2.6811e-01,\n",
       "                        -1.7554e-01, -2.8030e-01, -4.2757e-02, -6.7759e-02,  4.4282e-02,\n",
       "                        -1.5946e-01,  1.9407e-01,  3.6321e-02,  2.9000e-02,  3.5521e-01,\n",
       "                        -8.0872e-02,  1.1887e-02,  6.7295e-02, -1.1406e-02, -6.9654e-02,\n",
       "                         5.1059e-03, -5.5584e-02,  7.3080e-02, -3.7133e-01],\n",
       "                       [ 1.3101e-01, -1.7741e-01, -2.5593e-03,  1.2411e-01,  1.0175e-01,\n",
       "                        -1.1197e-01,  2.7972e-02, -3.1150e-01, -1.0673e-01, -1.5551e-01,\n",
       "                        -9.2293e-02, -4.5650e-02,  9.6367e-02,  2.9753e-01,  7.9677e-02,\n",
       "                         1.0232e-02,  5.7827e-03,  2.2479e-01,  3.4497e-02, -9.2442e-02,\n",
       "                         2.8327e-02,  1.9737e-02, -1.5509e-01,  7.8299e-02,  1.6079e-01,\n",
       "                         8.9281e-02, -1.4201e-01, -7.1407e-02, -2.4503e-01,  9.6893e-02,\n",
       "                        -1.7788e-01, -4.1735e-02,  7.3374e-02,  1.6377e-02, -1.5797e-01,\n",
       "                         2.1214e-01,  7.4874e-02, -4.4679e-02, -7.0878e-02,  2.0063e-01,\n",
       "                        -2.4742e-01,  3.3748e-02, -3.5117e-02, -1.2325e-01, -1.5401e-01,\n",
       "                         5.4235e-02, -5.0137e-02,  8.1214e-03,  1.2867e-01,  2.6206e-01,\n",
       "                         2.1035e-01,  3.2285e-02,  2.2584e-01,  1.6015e-02,  2.1067e-01,\n",
       "                         5.2365e-02, -6.0310e-02, -5.1032e-02,  6.8211e-02, -4.6471e-04,\n",
       "                        -5.9092e-02, -4.8653e-02, -6.1073e-02, -2.3168e-02, -4.4564e-02,\n",
       "                        -2.8243e-02, -1.9336e-01,  9.8784e-02, -1.2188e-01, -2.3719e-01,\n",
       "                         1.1872e-01, -3.8910e-02, -2.3011e-01, -5.7572e-02,  9.8255e-02,\n",
       "                        -1.6184e-02, -2.1480e-01,  6.6565e-02,  9.7874e-02, -2.4008e-01,\n",
       "                         3.2452e-01,  1.2918e-01, -3.9510e-02, -1.5814e-01]], device='cuda:2')),\n",
       "              ('fc3.bias',\n",
       "               tensor([-0.2610, -0.4755,  0.0904,  0.2727,  0.5934, -0.1263,  0.1962, -0.1718,\n",
       "                        0.1005, -0.3595], device='cuda:2'))]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = config['optimizer'](model.parameters(), lr=0.01, momentum=0.5)\n",
    "    \n",
    "wrapper.train(10, optimizer, 1, config['loss_fn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
